{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import pandas as pd\n",
    "from download import download_data_and_parse_it\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, drawings, labels):\n",
    "        self.drawings = drawings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.drawings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the drawing format to image\n",
    "        drawing = self.drawings.iloc[idx] if isinstance(self.drawings, pd.Series) else self.drawings[idx]\n",
    "        image = self.drawing_to_image(drawing)\n",
    "        label = self.labels.iloc[idx] if isinstance(self.labels, pd.Series) else self.labels[idx]\n",
    "        return torch.FloatTensor(image).unsqueeze(0), label\n",
    "    \n",
    "    def drawing_to_image(self, drawing):\n",
    "        # Create a blank 64x64 image\n",
    "        image = np.zeros((64, 64))\n",
    "        \n",
    "        # For each stroke in the drawing\n",
    "        for stroke in drawing:\n",
    "            # Get x and y coordinates\n",
    "            x = stroke[0]\n",
    "            y = stroke[1]\n",
    "            \n",
    "            # Draw lines between consecutive points\n",
    "            for i in range(len(x)-1):\n",
    "                x1, y1 = int(x[i]), int(y[i])\n",
    "                x2, y2 = int(x[i+1]), int(y[i+1])\n",
    "                \n",
    "                # Ensure points are within bounds\n",
    "                x1 = max(0, min(x1, 63))\n",
    "                y1 = max(0, min(y1, 63))\n",
    "                x2 = max(0, min(x2, 63))\n",
    "                y2 = max(0, min(y2, 63))\n",
    "                \n",
    "                # Draw line\n",
    "                image[y1, x1] = 255\n",
    "                image[y2, x2] = 255\n",
    "        \n",
    "        return image / 255.0\n",
    "    \n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists.\n",
      "         word countrycode                      timestamp  recognized  \\\n",
      "123792  sword          MY  2017-03-26 21:14:01.94413 UTC        True   \n",
      "123793  sword          PL  2017-03-27 14:06:53.35221 UTC        True   \n",
      "123794  sword          US  2017-03-17 18:29:46.34852 UTC        True   \n",
      "123795  sword          US    2017-01-27 14:54:45.687 UTC        True   \n",
      "123796  sword          SE   2017-01-27 09:38:24.6558 UTC        True   \n",
      "123797  sword          PL  2017-01-30 20:47:58.83752 UTC        True   \n",
      "123798  sword          DE  2017-01-27 13:56:28.54679 UTC        True   \n",
      "123799  sword          CA  2017-03-28 13:42:02.27022 UTC        True   \n",
      "123800  sword          DE  2017-03-12 09:35:38.31056 UTC        True   \n",
      "123801  sword          AU  2017-03-03 06:18:46.69524 UTC        True   \n",
      "\n",
      "                  key_id                                            drawing  \n",
      "123792  5927313763991552  [[[13, 30, 66], [52, 0, 59]], [[11, 26, 53, 76...  \n",
      "123793  6323617040171008  [[[38, 46, 56], [75, 86, 110]], [[46, 24, 48],...  \n",
      "123794  5246476118654976  [[[71, 95, 138, 156, 199, 200, 193, 131, 102],...  \n",
      "123795  6656148428029952  [[[165, 151, 117, 56, 40, 10, 1, 0, 15, 51], [...  \n",
      "123796  6722036581793792  [[[31, 10, 0, 29, 66], [202, 224, 241, 255, 21...  \n",
      "123797  4705338647379968  [[[94, 80, 50, 45, 45, 37], [3, 14, 51, 80, 13...  \n",
      "123798  5078634052190208  [[[0, 18, 64, 30, 16, 13, 16, 35, 52, 83, 100,...  \n",
      "123799  4633114221477888  [[[26, 97, 137, 156, 161, 158, 141, 121, 76, 9...  \n",
      "123800  6513417466675200  [[[66, 72, 174, 234, 241, 251, 254, 239, 138, ...  \n",
      "123801  6324727004004352  [[[54, 50, 53, 66, 71, 72, 103, 99, 96, 83, 72...  \n",
      "The file exists.\n",
      "        word countrycode                      timestamp  recognized  \\\n",
      "131517  tent          TW  2017-03-26 11:33:23.30803 UTC        True   \n",
      "131518  tent          NZ  2017-03-16 21:27:32.66622 UTC        True   \n",
      "131519  tent          US  2017-01-24 23:46:02.39221 UTC        True   \n",
      "131520  tent          US  2017-03-09 19:10:00.91201 UTC        True   \n",
      "131521  tent          DE  2017-01-27 15:19:29.09108 UTC        True   \n",
      "131522  tent          AE  2017-03-14 17:06:54.22769 UTC        True   \n",
      "131523  tent          GB  2017-03-14 10:30:10.85632 UTC        True   \n",
      "131524  tent          PH  2017-01-28 03:42:51.00346 UTC        True   \n",
      "131525  tent          RU  2017-01-25 20:34:06.93252 UTC        True   \n",
      "131526  tent          US  2017-01-26 00:32:48.95985 UTC        True   \n",
      "\n",
      "                  key_id                                            drawing  \n",
      "131517  5328916975714304  [[[109, 68, 0, 111, 153, 144, 101], [30, 77, 1...  \n",
      "131518  5294914759819264  [[[4, 62, 99, 116, 130, 128, 145, 170, 200, 21...  \n",
      "131519  4863834583990272  [[[70, 50, 20, 9, 1, 1, 4, 20, 74, 140, 192, 1...  \n",
      "131520  5163233159675904  [[[126, 49, 33, 128, 247, 253, 255, 246, 219, ...  \n",
      "131521  5605139194839040  [[[20, 50, 98, 121, 140, 167, 193, 199, 228], ...  \n",
      "131522  5537100046794752  [[[57, 55, 37, 26, 11, 8], [0, 78, 106, 133, 1...  \n",
      "131523  5159130257948672  [[[101, 45, 23, 3], [3, 159, 202, 230]], [[106...  \n",
      "131524  6579857058168832  [[[51, 94, 129, 135, 149, 176, 225, 196, 100, ...  \n",
      "131525  5776237941227520  [[[133, 132], [180, 181]], [[1, 3, 11, 21, 40,...  \n",
      "131526  4865932675514368  [[[0, 12, 24, 41, 53, 80, 92, 117, 127, 194, 2...  \n",
      "The file exists.\n",
      "        word countrycode                      timestamp  recognized  \\\n",
      "137609  star          US  2017-03-09 02:15:31.37731 UTC        True   \n",
      "137610  star          US  2017-01-28 19:53:08.59709 UTC        True   \n",
      "137611  star          IT  2017-01-27 19:27:41.85155 UTC        True   \n",
      "137612  star          US  2017-01-28 03:53:15.71915 UTC        True   \n",
      "137613  star          US  2017-03-08 02:15:13.19402 UTC        True   \n",
      "137614  star          US  2017-03-17 00:47:57.91579 UTC        True   \n",
      "137615  star          DE  2017-03-29 19:14:10.11319 UTC        True   \n",
      "137616  star          FI  2017-01-29 14:59:06.09068 UTC        True   \n",
      "137617  star          US  2017-03-23 03:38:30.79912 UTC        True   \n",
      "137618  star          CA  2017-03-28 00:38:35.24141 UTC        True   \n",
      "\n",
      "                  key_id                                            drawing  \n",
      "137609  5367371076206592  [[[176, 165, 163, 160, 151, 132, 130, 107, 79,...  \n",
      "137610  5195486589878272  [[[13, 13, 29, 57, 106, 114, 116, 141, 188, 21...  \n",
      "137611  5318981764251648  [[[59, 88, 100, 101, 99, 105, 137, 255, 213, 1...  \n",
      "137612  6620257399603200  [[[18, 48, 119, 144, 151, 149, 151, 151, 162, ...  \n",
      "137613  5951195308883968  [[[90, 95, 96, 114, 133, 146, 153, 221, 236, 2...  \n",
      "137614  5290529195556864  [[[63, 76, 85, 96, 105, 107, 111, 119, 129, 13...  \n",
      "137615  5098528957267968  [[[50, 107, 110, 110, 114, 119, 145, 154, 255,...  \n",
      "137616  5172681353723904  [[[5, 14, 61, 92, 137, 145, 158, 156, 153, 148...  \n",
      "137617  6543687272103936  [[[100, 117, 141, 152, 255, 254, 166, 211, 196...  \n",
      "137618  6365831803961344  [[[53, 0, 12, 33, 70, 106, 102, 102, 107, 135,...  \n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "# Load the JSON file into a DataFrame\n",
    "sword_dataset = download_data_and_parse_it(\"sword.ndjson\")\n",
    "tent_dataset = download_data_and_parse_it(\"tent.ndjson\")\n",
    "eiffel_dataset = download_data_and_parse_it(\"star.ndjson\")\n",
    "df = pd.concat([sword_dataset, tent_dataset])\n",
    "df = pd.concat([df,eiffel_dataset])\n",
    "df['class'] = le.fit_transform(df['word'])\n",
    "\n",
    "# Split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create datasets\n",
    "# train_dataset = QuickDrawDataset(train_df['drawing'], train_df['class'])\n",
    "# test_dataset = QuickDrawDataset(test_df['drawing'], test_df['class'])\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)  # 60% train, 40% temp\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # Split temp into 20% val, 20% test\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = QuickDrawDataset(train_df['drawing'], train_df['class'])\n",
    "val_dataset = QuickDrawDataset(val_df['drawing'], val_df['class'])\n",
    "test_dataset = QuickDrawDataset(test_df['drawing'], test_df['class'])\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):  # Number of epochs\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Testing loop\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6875787117982629\n",
      "Epoch 2, Loss: 0.6625695440808996\n",
      "Epoch 3, Loss: 0.6541292833024788\n",
      "Epoch 4, Loss: 0.6484258381668053\n",
      "Epoch 5, Loss: 0.6437222478177144\n",
      "Epoch 6, Loss: 0.6391067208073038\n",
      "Epoch 7, Loss: 0.6351148034066738\n",
      "Epoch 8, Loss: 0.6312979419840081\n",
      "Epoch 9, Loss: 0.62712627709373\n",
      "Epoch 10, Loss: 0.6237510833464539\n",
      "Accuracy: 72.23%\n",
      "Execution time: 418.718310 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_model(model)\n",
    "test_model(model)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-28 18:27:07,763] A new study created in memory with name: no-name-fbdc4e6e-18b1-4ebd-921c-05e4a7e38521\n",
      "[W 2024-12-28 18:27:31,366] Trial 0 failed with parameters: {'conv_layers': [16, 32, 64], 'kernel_size': 5, 'learning_rate': 0.0007850457369697414} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/quentin/BigData/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_913/1799750954.py\", line 130, in objective\n",
      "    _, val_accuracies = train_and_evaluate(config, train_loader, val_loader, device)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_913/1799750954.py\", line 91, in train_and_evaluate\n",
      "    for inputs, labels in train_loader:\n",
      "  File \"/home/quentin/BigData/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/quentin/BigData/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/quentin/BigData/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/tmp/ipykernel_913/1126338103.py\", line 12, in __getitem__\n",
      "    image = self.drawing_to_image(drawing)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_913/1126338103.py\", line 24, in drawing_to_image\n",
      "    x2, y2 = max(0, min(x2, 63)), max(0, min(y2, 63))\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-12-28 18:27:31,370] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 135\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Run Optuna study\u001b[39;00m\n\u001b[1;32m    134\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Retrieve best results\u001b[39;00m\n\u001b[1;32m    138\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[0;32m~/BigData/.venv/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BigData/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/BigData/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/BigData/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/BigData/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[35], line 130\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    127\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    128\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 130\u001b[0m _, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(val_accuracies)\n",
      "Cell \u001b[0;32mIn[35], line 91\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     89\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     90\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 91\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/BigData/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/BigData/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/BigData/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[29], line 12\u001b[0m, in \u001b[0;36mQuickDrawDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     11\u001b[0m     drawing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrawings\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[0;32m---> 12\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrawing_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrawing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mFloatTensor(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), label\n",
      "Cell \u001b[0;32mIn[29], line 24\u001b[0m, in \u001b[0;36mQuickDrawDataset.drawing_to_image\u001b[0;34m(self, drawing)\u001b[0m\n\u001b[1;32m     22\u001b[0m x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mint\u001b[39m(y[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     23\u001b[0m x1, y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmin\u001b[39m(x1, \u001b[38;5;241m63\u001b[39m)), \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmin\u001b[39m(y1, \u001b[38;5;241m63\u001b[39m))\n\u001b[0;32m---> 24\u001b[0m x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmin\u001b[39m(x2, \u001b[38;5;241m63\u001b[39m)), \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m63\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m image[y1, x1] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m     26\u001b[0m image[y2, x2] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the dataset (dummy data for illustration)\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, drawings, labels):\n",
    "        self.drawings = drawings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.drawings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        drawing = self.drawings.iloc[idx]\n",
    "        image = self.drawing_to_image(drawing)\n",
    "        label = self.labels.iloc[idx]\n",
    "        return torch.FloatTensor(image).unsqueeze(0), label\n",
    "\n",
    "    def drawing_to_image(self, drawing):\n",
    "        # Create a blank 64x64 image\n",
    "        image = np.zeros((64, 64))\n",
    "        \n",
    "        # For each stroke in the drawing\n",
    "        for stroke in drawing:\n",
    "            # Get x and y coordinates\n",
    "            x = stroke[0]\n",
    "            y = stroke[1]\n",
    "            \n",
    "            # Draw lines between consecutive points\n",
    "            for i in range(len(x)-1):\n",
    "                x1, y1 = int(x[i]), int(y[i])\n",
    "                x2, y2 = int(x[i+1]), int(y[i+1])\n",
    "                \n",
    "                # Ensure points are within bounds\n",
    "                x1 = max(0, min(x1, 63))\n",
    "                y1 = max(0, min(y1, 63))\n",
    "                x2 = max(0, min(x2, 63))\n",
    "                y2 = max(0, min(y2, 63))\n",
    "                \n",
    "                # Draw line\n",
    "                image[y1, x1] = 255\n",
    "                image[y2, x2] = 255\n",
    "\n",
    "# Define a dynamic model class\n",
    "def create_model(conv_layers, kernel_size, learning_rate, num_classes=10):\n",
    "    class CustomCNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CustomCNN, self).__init__()\n",
    "            layers = []\n",
    "            in_channels = 1\n",
    "            size = 64  # Initial image size\n",
    "            for out_channels in conv_layers:\n",
    "                layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.MaxPool2d(2))  # MaxPool2d halves the size\n",
    "                in_channels = out_channels\n",
    "                size = size // 2  # Each MaxPool2d halves the size\n",
    "            \n",
    "            self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "            # Dynamically calculate the output size after convolutions and pooling\n",
    "            # Pass a dummy tensor through the convolution layers to get the output size\n",
    "            with torch.no_grad():\n",
    "                dummy_input = torch.zeros(1, 1, 64, 64)  # (batch_size, channels, height, width)\n",
    "                output_size = self.conv_layers(dummy_input)\n",
    "                # Output size is [batch_size, channels, height, width]\n",
    "                final_size = output_size.view(1, -1).size(1)\n",
    "\n",
    "            # Fully connected layers\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(final_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, num_classes)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv_layers(x)\n",
    "            x = self.fc_layers(x)\n",
    "            return x\n",
    "\n",
    "    return CustomCNN()\n",
    "\n",
    "# Train and evaluate a model\n",
    "def train_and_evaluate(config, train_loader, val_loader, device):\n",
    "    model = create_model(config[\"conv_layers\"], config[\"kernel_size\"], config[\"learning_rate\"], num_classes=10).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses, val_accuracies = [], []\n",
    "    for epoch in range(5):  # Keep epochs small for testing\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_preds.extend(outputs.argmax(dim=1).tolist())\n",
    "                val_labels.extend(labels.tolist())\n",
    "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    conv_layers = trial.suggest_categorical(\"conv_layers\", [[16, 32], [16, 32, 64], [32, 64]])\n",
    "    kernel_size = trial.suggest_int(\"kernel_size\", 3, 5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    config = {\n",
    "        \"conv_layers\": conv_layers,\n",
    "        \"kernel_size\": kernel_size,\n",
    "        \"learning_rate\": learning_rate\n",
    "    }\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    _, val_accuracies = train_and_evaluate(config, train_loader, val_loader, device)\n",
    "    return max(val_accuracies)\n",
    "\n",
    "# Run Optuna study\n",
    "start_study_time = time.time()\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "end_study_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "# Retrieve best results\n",
    "best_trial = study.best_trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Visualization\n",
    "trials = study.trials\n",
    "val_accuracies = [trial.value for trial in trials]\n",
    "configs = [trial.params for trial in trials]\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(configs)), val_accuracies, tick_label=[f\"Trial {i + 1}\" for i in range(len(configs))])\n",
    "plt.title(\"Validation Accuracies Across Trials\")\n",
    "plt.xlabel(\"Trials\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
