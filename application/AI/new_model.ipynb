{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import pandas as pd\n",
    "from download import download_data_and_parse_it\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 12\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "size of datasets array : 12\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "datasets = [\n",
    "        \"star\", \"sword\", \"tent\", \"apple\", \"banana\", \"cat\", \n",
    "        \"dog\", \"car\", \"house\", \"tree\", \"guitar\", \"bicycle\"\n",
    "    ]\n",
    "    \n",
    "all_dfs = []  # List to store all DataFrames\n",
    "for dataset in datasets:\n",
    "    file_path = os.path.join(\"data\", f\"{dataset}.ndjson\")\n",
    "    dataset_df = download_data_and_parse_it(file_path)\n",
    "    all_dfs.append(dataset_df)\n",
    "# Combine all datasets\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Assign class labels using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['word'])\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)  # 60% train, 40% temp\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # Split temp into 20% val, 20% test\n",
    "\n",
    "print(\"size of datasets array :\", len(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Vector-To-Image Algorithm - Drawing-to-image changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, drawings, labels, resize_to=(64, 64)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            drawings (list or array): List of drawing data (tensor or numpy arrays).\n",
    "            labels (list or array): List of class labels corresponding to each drawing.\n",
    "            resize_to (tuple): Target size for resizing the image.\n",
    "        \"\"\"\n",
    "        self.drawings = drawings\n",
    "        self.labels = labels\n",
    "        self.resize_to = resize_to  # Tuple (width, height)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.drawings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the drawing format to image\n",
    "        drawing = self.drawings.iloc[idx] if isinstance(self.drawings, pd.Series) else self.drawings[idx]\n",
    "        image = self.drawing_to_image(drawing)\n",
    "        label = self.labels.iloc[idx] if isinstance(self.labels, pd.Series) else self.labels[idx]\n",
    "        \n",
    "        # Convert the image to a tensor and add batch dimension\n",
    "        image_tensor = torch.FloatTensor(image).unsqueeze(0)\n",
    "        return image_tensor, label\n",
    "\n",
    "    def drawing_to_image(self, drawing):\n",
    "        # Create a blank 256x256 image\n",
    "        img_size = 256\n",
    "        image = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "        \n",
    "        # Iterate through the strokes\n",
    "        for stroke in drawing:\n",
    "            x_coords = stroke[0]\n",
    "            y_coords = stroke[1]\n",
    "\n",
    "            # For each stroke, draw lines between consecutive points\n",
    "            for i in range(len(x_coords) - 1):\n",
    "                x1, y1 = int(x_coords[i]), int(y_coords[i])\n",
    "                x2, y2 = int(x_coords[i + 1]), int(y_coords[i + 1])\n",
    "\n",
    "                # Ensure that coordinates stay within bounds\n",
    "                x1 = max(0, min(x1, img_size - 1))\n",
    "                y1 = max(0, min(y1, img_size - 1))\n",
    "                x2 = max(0, min(x2, img_size - 1))\n",
    "                y2 = max(0, min(y2, img_size - 1))\n",
    "\n",
    "                # Draw line on image (we simply put the endpoints as pixels here)\n",
    "                image[y1, x1] = 255\n",
    "                image[y2, x2] = 255\n",
    "        \n",
    "        # Resize the image to the desired size (64x64)\n",
    "        pil_image = Image.fromarray(image)  # Convert to PIL Image\n",
    "        pil_image = pil_image.resize(self.resize_to, Image.Resampling.LANCZOS)  # Resize image\n",
    "        \n",
    "        # Convert back to numpy array\n",
    "        resized_image = np.array(pil_image)\n",
    "        return resized_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create datasets\n",
    "# train_dataset = QuickDrawDataset(train_df['drawing'], train_df['class'])\n",
    "# val_dataset = QuickDrawDataset(val_df['drawing'], val_df['class'])\n",
    "# test_dataset = QuickDrawDataset(test_df['drawing'], test_df['class'])\n",
    "\n",
    "# # Create DataLoaders\n",
    "# from torch.utils.data import DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the datasets with resizing\n",
    "train_dataset = QuickDrawDataset(train_df['drawing'], train_df['class'], resize_to=(64, 64))\n",
    "val_dataset = QuickDrawDataset(val_df['drawing'], val_df['class'], resize_to=(64, 64))\n",
    "test_dataset = QuickDrawDataset(test_df['drawing'], test_df['class'], resize_to=(64, 64))\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32  # Example batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to visualize some entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), np.int64(6))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drawing = train_dataset.__getitem__(15000)\n",
    "\n",
    "print(drawing)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the image\n",
    "plt.imshow(drawing[0].squeeze(), cmap='gray')  # Squeeze to remove the single channel dimension\n",
    "plt.axis('off')  # Hide axis for better visualization\n",
    "plt.title(drawing[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drawing = [[[48.22564697265625,45.22564697265625,40.22564697265625,37.22564697265625,33.22564697265625,30.22564697265625,28.22564697265625,25.22564697265625,25.22564697265625,25.22564697265625,25.22564697265625,26.22564697265625,27.22564697265625,28.22564697265625,31.22564697265625,31.22564697265625,35.22564697265625,37.22564697265625,39.22564697265625,42.22564697265625,44.22564697265625,46.22564697265625,48.22564697265625,50.22564697265625,52.22564697265625,52.22564697265625,54.22564697265625,54.22564697265625,54.22564697265625,55.22564697265625,55.22564697265625,56.22564697265625,56.22564697265625,56.22564697265625,56.22564697265625,55.22564697265625,53.22564697265625,53.22564697265625,52.22564697265625,52.22564697265625,50.22564697265625,49.22564697265625,48.22564697265625,47.22564697265625,46.22564697265625,45.22564697265625,45.22564697265625],[157.2708282470703,156.2708282470703,156.2708282470703,155.2708282470703,152.2708282470703,150.2708282470703,145.2708282470703,140.2708282470703,138.2708282470703,135.2708282470703,134.2708282470703,131.2708282470703,130.2708282470703,129.2708282470703,129.2708282470703,129.2708282470703,129.2708282470703,129.2708282470703,130.2708282470703,131.2708282470703,131.2708282470703,132.2708282470703,134.2708282470703,134.2708282470703,136.2708282470703,136.2708282470703,139.2708282470703,140.2708282470703,143.2708282470703,144.2708282470703,146.2708282470703,147.2708282470703,147.2708282470703,149.2708282470703,150.2708282470703,152.2708282470703,154.2708282470703,155.2708282470703,156.2708282470703,157.2708282470703,158.2708282470703,158.2708282470703,158.2708282470703,158.2708282470703,158.2708282470703,157.2708282470703,157.2708282470703],[1,3,21,39,57,75,93,111,129,147,165,182,200,217,235,252,270,289,307,324,343,361,379,396,415,433,451,468,487,505,522,541,559,576,593,611,629,646,670,691,736,758,772,789,807,825,843]],[[185.22564697265625,182.22564697265625,180.22564697265625,176.22564697265625,175.22564697265625,174.22564697265625,174.22564697265625,175.22564697265625,176.22564697265625,176.22564697265625,177.22564697265625,178.22564697265625,178.22564697265625,179.22564697265625,182.22564697265625,186.22564697265625,189.22564697265625,191.22564697265625,193.22564697265625,198.22564697265625,199.22564697265625,200.22564697265625,201.22564697265625,202.22564697265625,202.22564697265625,202.22564697265625,202.22564697265625,202.22564697265625,202.22564697265625,202.22564697265625,201.22564697265625,201.22564697265625,200.22564697265625,199.22564697265625,198.22564697265625,197.22564697265625,195.22564697265625,195.22564697265625,194.22564697265625,193.22564697265625,192.22564697265625,191.22564697265625,189.22564697265625,187.22564697265625,185.22564697265625,183.22564697265625],[161.2708282470703,160.2708282470703,157.2708282470703,152.2708282470703,150.2708282470703,149.2708282470703,146.2708282470703,139.2708282470703,135.2708282470703,135.2708282470703,132.2708282470703,132.2708282470703,132.2708282470703,132.2708282470703,132.2708282470703,131.2708282470703,131.2708282470703,132.2708282470703,133.2708282470703,136.2708282470703,136.2708282470703,137.2708282470703,139.2708282470703,139.2708282470703,140.2708282470703,141.2708282470703,144.2708282470703,145.2708282470703,148.2708282470703,151.2708282470703,155.2708282470703,155.2708282470703,157.2708282470703,157.2708282470703,158.2708282470703,159.2708282470703,161.2708282470703,161.2708282470703,161.2708282470703,161.2708282470703,161.2708282470703,161.2708282470703,161.2708282470703,161.2708282470703,161.2708282470703,161.2708282470703],[1589,1603,1621,1639,1657,1674,1692,1710,1728,1746,1764,1787,1806,1819,1837,1855,1873,1891,1911,1928,1947,1965,1984,2001,2020,2038,2056,2074,2091,2109,2127,2145,2164,2182,2205,2217,2235,2253,2271,2288,2306,2325,2343,2361,2380,2386]],[[21.22564697265625,22.22564697265625,23.22564697265625,25.22564697265625,26.22564697265625,27.22564697265625,28.22564697265625,30.22564697265625,31.22564697265625,33.22564697265625,37.22564697265625,41.22564697265625,46.22564697265625,49.22564697265625,55.22564697265625,57.22564697265625,60.22564697265625,62.22564697265625,66.22564697265625,67.22564697265625,67.22564697265625,67.22564697265625,67.22564697265625,67.22564697265625,66.22564697265625,65.22564697265625,65.22564697265625,64.22564697265625,63.22564697265625,62.22564697265625,62.22564697265625,63.22564697265625,63.22564697265625,65.22564697265625,68.22564697265625,74.22564697265625,76.22564697265625,85.22564697265625,87.22564697265625,92.22564697265625,96.22564697265625,100.22564697265625,103.22564697265625,106.22564697265625,110.22564697265625,112.22564697265625,116.22564697265625,122.22564697265625,127.22564697265625,130.22564697265625,133.22564697265625,140.22564697265625,143.22564697265625,146.22564697265625,148.22564697265625,149.22564697265625,150.22564697265625,151.22564697265625,153.22564697265625,153.22564697265625,154.22564697265625,154.22564697265625,154.22564697265625,155.22564697265625,156.22564697265625,158.22564697265625,160.22564697265625,160.22564697265625,161.22564697265625,162.22564697265625,163.22564697265625,165.22564697265625,165.22564697265625,166.22564697265625,166.22564697265625,167.22564697265625,168.22564697265625,169.22564697265625,170.22564697265625,172.22564697265625,172.22564697265625,173.22564697265625,176.22564697265625,177.22564697265625,178.22564697265625,178.22564697265625,180.22564697265625,180.22564697265625,182.22564697265625,182.22564697265625,184.22564697265625,186.22564697265625,187.22564697265625,189.22564697265625,190.22564697265625,191.22564697265625,195.22564697265625,195.22564697265625,197.22564697265625,198.22564697265625,202.22564697265625,207.22564697265625,208.22564697265625,212.22564697265625,215.22564697265625,216.22564697265625,216.22564697265625,217.22564697265625,218.22564697265625,218.22564697265625,218.22564697265625,218.22564697265625,218.22564697265625,218.22564697265625,220.22564697265625,223.22564697265625,225.22564697265625,228.22564697265625,230.22564697265625,230.22564697265625,231.22564697265625,232.22564697265625,232.22564697265625,231.22564697265625,231.22564697265625,230.22564697265625,230.22564697265625,230.22564697265625,230.22564697265625,230.22564697265625,230.22564697265625,227.22564697265625,226.22564697265625,226.22564697265625,224.22564697265625,222.22564697265625,222.22564697265625,221.22564697265625,218.22564697265625,217.22564697265625,215.22564697265625,213.22564697265625,212.22564697265625,211.22564697265625,210.22564697265625,202.22564697265625,193.22564697265625,191.22564697265625,188.22564697265625,186.22564697265625,178.22564697265625,174.22564697265625,169.22564697265625,165.22564697265625,161.22564697265625,155.22564697265625,150.22564697265625,147.22564697265625,145.22564697265625,145.22564697265625,145.22564697265625,144.22564697265625,144.22564697265625,143.22564697265625,141.22564697265625,141.22564697265625,141.22564697265625,140.22564697265625,140.22564697265625,140.22564697265625,140.22564697265625,140.22564697265625,138.22564697265625,136.22564697265625,135.22564697265625,122.22564697265625,120.22564697265625,116.22564697265625,110.22564697265625,102.22564697265625,98.22564697265625,95.22564697265625,91.22564697265625,90.22564697265625,89.22564697265625,86.22564697265625,85.22564697265625,84.22564697265625,83.22564697265625,82.22564697265625,80.22564697265625,80.22564697265625,78.22564697265625,78.22564697265625,77.22564697265625,77.22564697265625,78.22564697265625,78.22564697265625,78.22564697265625,78.22564697265625,78.22564697265625,79.22564697265625,79.22564697265625,79.22564697265625,78.22564697265625,77.22564697265625,72.22564697265625,66.22564697265625,60.22564697265625,56.22564697265625,55.22564697265625,53.22564697265625,52.22564697265625,46.22564697265625,41.22564697265625,38.22564697265625,36.22564697265625,33.22564697265625,31.22564697265625,28.22564697265625,26.22564697265625,21.22564697265625,18.22564697265625,18.22564697265625,18.22564697265625,18.22564697265625,22.22564697265625,23.22564697265625,24.22564697265625,26.22564697265625,27.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625,28.22564697265625],[145.2708282470703,133.2708282470703,128.2708282470703,122.27082824707031,119.27082824707031,117.27082824707031,114.27082824707031,112.27082824707031,111.27082824707031,110.27082824707031,110.27082824707031,110.27082824707031,111.27082824707031,112.27082824707031,115.27082824707031,117.27082824707031,118.27082824707031,120.27082824707031,122.27082824707031,123.27082824707031,124.27082824707031,125.27082824707031,127.27082824707031,130.2708282470703,132.2708282470703,134.2708282470703,134.2708282470703,135.2708282470703,136.2708282470703,136.2708282470703,138.2708282470703,139.2708282470703,140.2708282470703,140.2708282470703,140.2708282470703,140.2708282470703,140.2708282470703,139.2708282470703,139.2708282470703,138.2708282470703,137.2708282470703,136.2708282470703,135.2708282470703,135.2708282470703,134.2708282470703,134.2708282470703,134.2708282470703,134.2708282470703,135.2708282470703,135.2708282470703,135.2708282470703,137.2708282470703,137.2708282470703,139.2708282470703,139.2708282470703,139.2708282470703,139.2708282470703,139.2708282470703,140.2708282470703,140.2708282470703,140.2708282470703,138.2708282470703,135.2708282470703,131.2708282470703,131.2708282470703,127.27082824707031,126.27082824707031,124.27082824707031,122.27082824707031,121.27082824707031,117.27082824707031,116.27082824707031,114.27082824707031,114.27082824707031,113.27082824707031,113.27082824707031,112.27082824707031,111.27082824707031,111.27082824707031,110.27082824707031,110.27082824707031,110.27082824707031,110.27082824707031,109.27082824707031,107.27082824707031,107.27082824707031,107.27082824707031,108.27082824707031,110.27082824707031,111.27082824707031,112.27082824707031,114.27082824707031,115.27082824707031,116.27082824707031,118.27082824707031,119.27082824707031,120.27082824707031,120.27082824707031,121.27082824707031,122.27082824707031,123.27082824707031,126.27082824707031,126.27082824707031,129.2708282470703,131.2708282470703,134.2708282470703,135.2708282470703,136.2708282470703,138.2708282470703,139.2708282470703,140.2708282470703,141.2708282470703,143.2708282470703,143.2708282470703,143.2708282470703,142.2708282470703,141.2708282470703,141.2708282470703,140.2708282470703,140.2708282470703,140.2708282470703,139.2708282470703,139.2708282470703,132.2708282470703,131.2708282470703,121.27082824707031,120.27082824707031,115.27082824707031,112.27082824707031,109.27082824707031,107.27082824707031,104.27082824707031,101.27082824707031,100.27082824707031,95.27082824707031,93.27082824707031,91.27082824707031,90.27082824707031,89.27082824707031,88.27082824707031,87.27082824707031,86.27082824707031,85.27082824707031,85.27082824707031,85.27082824707031,85.27082824707031,85.27082824707031,84.27082824707031,84.27082824707031,84.27082824707031,84.27082824707031,84.27082824707031,84.27082824707031,84.27082824707031,82.27082824707031,81.27082824707031,80.27082824707031,80.27082824707031,79.27082824707031,79.27082824707031,79.27082824707031,76.27082824707031,75.27082824707031,71.27082824707031,67.27082824707031,65.27082824707031,61.27082824707031,58.27082824707031,53.27082824707031,51.27082824707031,50.27082824707031,49.27082824707031,49.27082824707031,49.27082824707031,49.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,50.27082824707031,51.27082824707031,51.27082824707031,52.27082824707031,52.27082824707031,54.27082824707031,58.27082824707031,61.27082824707031,69.27082824707031,71.27082824707031,75.27082824707031,81.27082824707031,83.27082824707031,84.27082824707031,84.27082824707031,85.27082824707031,86.27082824707031,86.27082824707031,87.27082824707031,87.27082824707031,87.27082824707031,87.27082824707031,87.27082824707031,87.27082824707031,87.27082824707031,89.27082824707031,90.27082824707031,90.27082824707031,90.27082824707031,91.27082824707031,91.27082824707031,91.27082824707031,91.27082824707031,92.27082824707031,92.27082824707031,93.27082824707031,94.27082824707031,96.27082824707031,105.27082824707031,109.27082824707031,112.27082824707031,116.27082824707031,120.27082824707031,124.27082824707031,125.27082824707031,127.27082824707031,127.27082824707031,128.2708282470703,129.2708282470703,129.2708282470703,131.2708282470703,133.2708282470703,135.2708282470703,137.2708282470703,138.2708282470703,139.2708282470703,140.2708282470703,140.2708282470703,141.2708282470703,142.2708282470703],[3125,3135,3152,3169,3187,3205,3223,3241,3259,3277,3295,3312,3330,3348,3366,3385,3402,3420,3438,3456,3476,3492,3510,3528,3545,3563,3581,3599,3617,3635,3661,3678,3689,3707,3725,3743,3761,3778,3796,3814,3832,3850,3868,3887,3904,3922,3941,3958,3976,3994,4012,4030,4048,4066,4084,4102,4131,4138,4156,4174,4192,4287,4306,4324,4343,4361,4379,4397,4415,4432,4450,4468,4486,4504,4522,4550,4567,4576,4594,4612,4630,4649,4667,4684,4703,4726,4739,4757,4775,4796,4811,4828,4846,4864,4882,4901,4918,4936,4954,4972,4990,5008,5026,5044,5062,5080,5098,5116,5135,5153,5171,5190,5243,5408,5426,5444,5463,5482,5500,5557,5575,5672,5691,5709,5728,5746,5765,5782,5801,5819,5837,5855,5873,5891,5909,5927,5945,5962,5980,5998,6016,6034,6052,6071,6089,6107,6125,6143,6161,6178,6197,6214,6232,6250,6268,6286,6305,6322,6341,6359,6416,6434,6452,6470,6488,6506,6524,6542,6560,6578,6596,6736,6754,6772,6790,6808,6832,6844,6862,6879,6897,6916,6934,6952,7006,7023,7041,7059,7078,7096,7114,7132,7151,7168,7186,7205,7223,7242,7259,7277,7295,7313,7423,7449,7460,7477,7495,7513,7531,7550,7567,7586,7603,7621,7639,7657,7675,7694,7712,7730,7749,7767,7785,7897,7915,7933,7951,7969,7986,8004,8021,8039,8056,8074,8093,8118,8137,8146,8164,8182,8200,8218,8236,8254,8272,8290,8375,8393]]]\n",
    "reconstructed_image = train_dataset.drawing_to_image(drawing)\n",
    "plt.imshow(reconstructed_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Example input data (simplified vector data from the NDJSON)\n",
    "# drawings = [\n",
    "#     [[[4,18,29,63,93,120,146,169,186,218,244,234,186,154,128,86,44,14,0],[7,51,66,90,101,106,106,101,93,67,22,23,49,58,59,53,26,16,6]],[[10,27,42,78,135,162,212,230,244],[15,39,53,67,80,74,48,35,20]],[[9,2,16,22,23,20],[18,3,0,3,8,20]],[[229,244,254,252,241],[23,17,18,22,30]],[[52,52],[52,52]],[[52,50],[52,52]],[[59,43],[69,61]]],\n",
    "#     [[[223,226,227,233,254,255,248,227],[35,25,2,0,2,21,27,30]],[[234,235,244,246,249],[4,8,12,19,20]],[[232,208,168,135,98,37],[28,51,104,138,164,196]],[[255,255,245,194,182,172,115,26,18,8],[25,61,106,199,213,217,219,235,235,230]],[[12,1,0,0,10,22,29,28,16,11],[193,191,194,206,219,218,206,199,194,197]],[[41,29,12],[232,228,217]],[[7,7,12,14,14,18,19,23,27,29,37],[186,199,190,193,209,200,204,204,194,203,192]],[[230,230,235,238,241,245,245],[8,15,8,17,16,4,11]],[[251,251,241,235,211,191,121,59,31],[26,69,113,126,155,168,200,215,215]],[[214,201,177,153,120,108],[72,102,131,152,167,175]]], \n",
    "#     [[[2,0,4,24,41,94,160,186,189,177,151,134,94,71,30],[0,38,79,138,162,207,247,255,251,238,218,200,139,98,45]]]\n",
    "# ]\n",
    "\n",
    "# # Reconstruct the image from the simplified drawing\n",
    "# index = 0\n",
    "# for drawing in drawings:\n",
    "#     reconstructed_image = train_dataset.drawing_to_image(drawing)\n",
    "#     index+=1\n",
    "#     # Display the image\n",
    "#     plt.imshow(reconstructed_image, cmap='gray')\n",
    "#     plt.title(index)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just a resizing test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `drawing` is the tuple as described\n",
    "image_tensor = drawing[0]  # Image tensor of shape (1, H, W)\n",
    "label = drawing[1]         # The label (in this case, 3)\n",
    "\n",
    "# Convert the image to 3D (H, W, C) for easier manipulation\n",
    "image_numpy = image_tensor.squeeze().cpu().numpy()\n",
    "\n",
    "# Resize the image to a smaller size using a transformation (e.g., 128x128)\n",
    "resize_transform = T.Resize((64, 64))  # Resize to 128x128\n",
    "resized_image_tensor = resize_transform(image_tensor.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "# Convert back to numpy array for visualization\n",
    "resized_image_numpy = resized_image_tensor.squeeze().cpu().numpy()\n",
    "\n",
    "# Visualize the original and resized image\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original image\n",
    "ax[0].imshow(image_numpy, cmap='gray')\n",
    "ax[0].set_title(\"Original Image (256x256)\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "# Resized image\n",
    "ax[1].imshow(resized_image_numpy, cmap='gray')\n",
    "ax[1].set_title(\"Resized Image (64x64)\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):  # Number of epochs\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Testing loop\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16*16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model structure SimpleCNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=8192, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1, Loss: 0.4110197599155996\n",
      "Epoch 2, Loss: 0.31496348395428075\n",
      "Epoch 3, Loss: 0.2823221774107087\n",
      "Epoch 4, Loss: 0.259060027471739\n",
      "Epoch 5, Loss: 0.23983934732956932\n",
      "Epoch 6, Loss: 0.22329707152937295\n",
      "Epoch 7, Loss: 0.2094609522234722\n",
      "Epoch 8, Loss: 0.19705468979870466\n",
      "Epoch 9, Loss: 0.18663024016773255\n",
      "Epoch 10, Loss: 0.1772725153415165\n",
      "Accuracy: 89.34%\n",
      "Execution time: 8612.358917 seconds\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"model structure\", model)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(model, optimizer, criterion)\n",
    "test_model(model)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to best_model2.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "model_file_path = \"best_model2.pth\"\n",
    "torch.save(model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightModel structure EfficientCNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1, Loss: 0.4318207372455258\n",
      "Epoch 2, Loss: 0.338471194588679\n",
      "Epoch 3, Loss: 0.30951562185115966\n",
      "Epoch 4, Loss: 0.29060624745590086\n",
      "Epoch 5, Loss: 0.27626049889245025\n",
      "Epoch 6, Loss: 0.2647074138012333\n",
      "Epoch 7, Loss: 0.25459925987843174\n",
      "Epoch 8, Loss: 0.24577768132687794\n",
      "Epoch 9, Loss: 0.23832450930444152\n",
      "Epoch 10, Loss: 0.231188982910864\n",
      "Accuracy: 89.33%\n",
      "Execution time: 8956.930888 seconds\n"
     ]
    }
   ],
   "source": [
    "class EfficientCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EfficientCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # 1 input channel, 16 filters\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Reduce size by half\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16 * 16, 64),  # Fewer neurons in FC layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "lightModel = EfficientCNN(num_classes).to(device)\n",
    "optimizer = optim.Adam(lightModel.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"lightModel structure\", lightModel)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(lightModel, optimizer, criterion)\n",
    "test_model(lightModel)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),  # Normalize feature maps for faster convergence\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Add a third convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 256),  # More neurons for higher capacity\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Add dropout for regularization\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "deepModel = EnhancedCNN(num_classes).to(device)\n",
    "# optimizer = optim.Adam(deepModel.parameters(), lr=learning_rate)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# print(\"lightModel structure\", deepModel)\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# train_model(deepModel, optimizer, criterion)\n",
    "# test_model(deepModel)\n",
    "\n",
    "# end_time = time.time()\n",
    "\n",
    "# execution_time = end_time - start_time\n",
    "# print(f\"Execution time: {execution_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to deepest_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "model_file_path = \"deepest_model.pth\"\n",
    "torch.save(deepModel.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two deeper models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_model_with_scheduler(model, optimizer, scheduler, criterion, num_epochs=10):\n",
    "    print(num_epochs)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning Rate: {param_group['lr']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetLikeCNN structure:\n",
      "ResNetLikeCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (shortcut): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(output_size=(8, 8))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/10], Loss: 0.5782\n",
      "Epoch [2/10], Loss: 0.4584\n",
      "Epoch [3/10], Loss: 0.4262\n",
      "Epoch [4/10], Loss: 0.4053\n",
      "Epoch [5/10], Loss: 0.3914\n",
      "Epoch [6/10], Loss: 0.3814\n",
      "Epoch [7/10], Loss: 0.3732\n",
      "Epoch [8/10], Loss: 0.3651\n",
      "Epoch [9/10], Loss: 0.3596\n",
      "Epoch [10/10], Loss: 0.3542\n",
      "Accuracy: 90.80%\n",
      "Execution time: 14662.274112 seconds\n",
      "Model saved to deeper1_model_dynamic.pth\n"
     ]
    }
   ],
   "source": [
    "class ResNetLikeCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetLikeCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Shortcut connection to match the channel dimensions\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=1),  # Match input to output channels\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Ensure consistent spatial dimensions\n",
    "        self.fc = None  # Placeholder, will be dynamically initialized\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))  # First convolution\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))  # Second convolution\n",
    "        residual = self.shortcut(x)              # Adjust dimensions for residual\n",
    "        x = torch.relu(self.bn3(self.conv3(x)) + residual)  # Add residual to output\n",
    "        x = self.pool(x)  # Downsample to fixed size\n",
    "        x = torch.flatten(x, 1)  # Flatten before fully connected layers\n",
    "        x = self.fc(x)  # Fully connected layers\n",
    "        return x\n",
    "\n",
    "    def initialize_fc(self, input_shape):\n",
    "        # Dynamically calculate the input size of the flattened feature map\n",
    "        dummy_input = torch.zeros(input_shape)  # Create a dummy tensor with input shape\n",
    "        conv_output = self.conv1(dummy_input)  # Pass through layers to determine flattened size\n",
    "        conv_output = torch.relu(self.bn1(conv_output))\n",
    "        conv_output = torch.relu(self.bn2(self.conv2(conv_output)))\n",
    "        residual = self.shortcut(conv_output)\n",
    "        conv_output = torch.relu(self.bn3(self.conv3(conv_output)) + residual)\n",
    "        conv_output = self.pool(conv_output)\n",
    "        flattened_size = torch.flatten(conv_output, 1).shape[1]  # Calculate flattened size\n",
    "        \n",
    "        # Define the fully connected layers dynamically\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "# Define number of classes and input shape\n",
    "input_shape = (1, 1, 64, 64)  # Batch size of 1, 1 channel, 64x64 image size\n",
    "\n",
    "# Instantiate the model and initialize fully connected layers\n",
    "deeper1Model = ResNetLikeCNN(num_classes=num_classes)\n",
    "deeper1Model.initialize_fc(input_shape)\n",
    "\n",
    "# Move model to the device\n",
    "deeper1Model = deeper1Model.to(device)\n",
    "\n",
    "# Define optimizer, scheduler, and criterion\n",
    "optimizer = optim.Adam(deeper1Model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print model structure\n",
    "print(\"ResNetLikeCNN structure:\")\n",
    "print(deeper1Model)\n",
    "\n",
    "# Training and testing\n",
    "start_time = time.time()\n",
    "train_model(deeper1Model, optimizer, scheduler, criterion, num_epochs)\n",
    "test_model(deeper1Model)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "# Save the trained model\n",
    "model_file_path = \"deeper1_model_dynamic.pth\"\n",
    "torch.save(deeper1Model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeeperCNN2 structure:\n",
      "DeeperCNN2(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
      "  (fc1): Linear(in_features=8192, out_features=512, bias=True)\n",
      ")\n",
      "Epoch [1/10], Loss: 0.4659\n",
      "Epoch [2/10], Loss: 0.3979\n",
      "Epoch [3/10], Loss: 0.3839\n",
      "Epoch [4/10], Loss: 0.3762\n",
      "Epoch [5/10], Loss: 0.3717\n",
      "Epoch [6/10], Loss: 0.3676\n",
      "Epoch [7/10], Loss: 0.3643\n",
      "Epoch [8/10], Loss: 0.3627\n",
      "Epoch [9/10], Loss: 0.3605\n",
      "Epoch [10/10], Loss: 0.3589\n",
      "Accuracy: 88.95%\n",
      "Execution time: 9541.829543 seconds\n",
      "Model saved to deeper2_model_dynamic.pth\n"
     ]
    }
   ],
   "source": [
    "class DeeperCNN2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeeperCNN2, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc1 = None  # Placeholder, will be initialized dynamically\n",
    "        self.fc2 = nn.Linear(512, num_classes)  # Output layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)  # Pass through convolutional layers\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch size\n",
    "        x = self.fc1(x)  # First fully connected layer\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "    def initialize_fc(self, input_shape):\n",
    "        # Dynamically calculate the input size of the flattened feature map\n",
    "        dummy_input = torch.zeros(input_shape)  # Create a dummy tensor with input shape\n",
    "        conv_output = self.conv_layers(dummy_input)  # Pass through conv layers\n",
    "        flattened_size = torch.flatten(conv_output, 1).shape[1]  # Calculate flattened size\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)  # Initialize fc1 with calculated size\n",
    "\n",
    "\n",
    "input_shape = (1, 1, 64, 64)  # Batch size of 1, 1 channel, 64x64 image size\n",
    "\n",
    "# Instantiate the model and initialize fully connected layer\n",
    "deeper2Model = DeeperCNN2(num_classes=num_classes)\n",
    "deeper2Model.initialize_fc(input_shape)\n",
    "\n",
    "# Move model to the device\n",
    "deeper2Model = deeper2Model.to(device)\n",
    "\n",
    "# Define optimizer, scheduler, and criterion\n",
    "optimizer = optim.Adam(deeper2Model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print model structure\n",
    "print(\"DeeperCNN2 structure:\")\n",
    "print(deeper2Model)\n",
    "\n",
    "# Training and testing\n",
    "start_time = time.time()\n",
    "train_model(deeper2Model, optimizer, scheduler, criterion, num_epochs)\n",
    "test_model(deeper2Model)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "# Save the trained model\n",
    "model_file_path = \"deeper2_model_dynamic.pth\"\n",
    "torch.save(deeper2Model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20213/2596780188.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"deepest_model.pth\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 12\n",
    "model = EnhancedCNN(num_classes)\n",
    "model.load_state_dict(torch.load(\"deepest_model.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "transform = ToTensor()\n",
    "input_image = transform(reconstructed_image).unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
