{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import pandas as pd\n",
    "from download import download_data_and_parse_it\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 12\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "The file exists.\n",
      "[['apple' 0]\n",
      " ['banana' 1]\n",
      " ['bicycle' 2]\n",
      " ['car' 3]\n",
      " ['cat' 4]\n",
      " ['dog' 5]\n",
      " ['guitar' 6]\n",
      " ['house' 7]\n",
      " ['star' 8]\n",
      " ['sword' 9]\n",
      " ['tent' 10]\n",
      " ['tree' 11]]\n",
      "size of datasets array : 12\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "datasets = [\n",
    "        \"star\", \"sword\", \"tent\", \"apple\", \"banana\", \"cat\", \n",
    "        \"dog\", \"car\", \"house\", \"tree\", \"guitar\", \"bicycle\"\n",
    "    ]\n",
    "    \n",
    "all_dfs = []  # List to store all DataFrames\n",
    "for dataset in datasets:\n",
    "    file_path = os.path.join(\"data\", f\"{dataset}.ndjson\")\n",
    "    dataset_df = download_data_and_parse_it(file_path)\n",
    "    all_dfs.append(dataset_df)\n",
    "# Combine all datasets\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Assign class labels using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['word'])\n",
    "\n",
    "word_class_mapping = df[['word', 'class']].drop_duplicates().sort_values('class').to_numpy()\n",
    "\n",
    "# Display the array\n",
    "print(word_class_mapping)\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)  # 60% train, 40% temp\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # Split temp into 20% val, 20% test\n",
    "\n",
    "print(\"size of datasets array :\", len(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Vector-To-Image Algorithm - Drawing-to-image changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, drawings, labels, resize_to=(64, 64)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            drawings (list or array): List of drawing data (tensor or numpy arrays).\n",
    "            labels (list or array): List of class labels corresponding to each drawing.\n",
    "            resize_to (tuple): Target size for resizing the image.\n",
    "        \"\"\"\n",
    "        self.drawings = drawings\n",
    "        self.labels = labels\n",
    "        self.resize_to = resize_to  # Tuple (width, height)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.drawings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the drawing format to image\n",
    "        drawing = self.drawings.iloc[idx] if isinstance(self.drawings, pd.Series) else self.drawings[idx]\n",
    "        image = self.drawing_to_image(drawing)\n",
    "        label = self.labels.iloc[idx] if isinstance(self.labels, pd.Series) else self.labels[idx]\n",
    "        \n",
    "        # Convert the image to a tensor and add batch dimension\n",
    "        image_tensor = torch.FloatTensor(image).unsqueeze(0)\n",
    "        return image_tensor, label\n",
    "\n",
    "    def drawing_to_image(self, drawing):\n",
    "        # Create a blank 256x256 image\n",
    "        img_size = 512\n",
    "        image = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "        \n",
    "        # Iterate through the strokes\n",
    "        for stroke in drawing:\n",
    "            x_coords = stroke[0]\n",
    "            y_coords = stroke[1]\n",
    "\n",
    "            # For each stroke, draw lines between consecutive points\n",
    "            for i in range(len(x_coords) - 1):\n",
    "                x1, y1 = int(x_coords[i]), int(y_coords[i])\n",
    "                x2, y2 = int(x_coords[i + 1]), int(y_coords[i + 1])\n",
    "\n",
    "                # Ensure that coordinates stay within bounds\n",
    "                x1 = max(0, min(x1, img_size - 1))\n",
    "                y1 = max(0, min(y1, img_size - 1))\n",
    "                x2 = max(0, min(x2, img_size - 1))\n",
    "                y2 = max(0, min(y2, img_size - 1))\n",
    "\n",
    "                # Draw line on image (we simply put the endpoints as pixels here)\n",
    "                image[y1, x1] = 255\n",
    "                image[y2, x2] = 255\n",
    "        \n",
    "        # Resize the image to the desired size (64x64)\n",
    "        pil_image = Image.fromarray(image)  # Convert to PIL Image\n",
    "        pil_image = pil_image.resize(self.resize_to, Image.Resampling.LANCZOS)  # Resize image\n",
    "        \n",
    "        # Convert back to numpy array\n",
    "        resized_image = np.array(pil_image)\n",
    "        return resized_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create datasets\n",
    "# train_dataset = QuickDrawDataset(train_df['drawing'], train_df['class'])\n",
    "# val_dataset = QuickDrawDataset(val_df['drawing'], val_df['class'])\n",
    "# test_dataset = QuickDrawDataset(test_df['drawing'], test_df['class'])\n",
    "\n",
    "# # Create DataLoaders\n",
    "# from torch.utils.data import DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the datasets with resizing\n",
    "train_dataset = QuickDrawDataset(train_df['drawing'], train_df['class'], resize_to=(64, 64))\n",
    "val_dataset = QuickDrawDataset(val_df['drawing'], val_df['class'], resize_to=(64, 64))\n",
    "test_dataset = QuickDrawDataset(test_df['drawing'], test_df['class'], resize_to=(64, 64))\n",
    "\n",
    "# Create DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32  # Example batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to visualize some entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [2., 4., 3.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), np.int64(3))\n",
      "torch.Size([1, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAACG1JREFUeJzt3bFrldcfgPGjFqHSboWC0qWYpWt2x4IOGURxdLNTXPw7ukRBHLoUCkVwqWD+huIoQjAugogUCtFiLRRJt6c/7L1wL79432A+n/FwuTlwAw8355vzHtvf398fADDGOD71BgA4PEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAkfW48ePx+XLl8fXX389Tp06Nb744otx7ty5cf/+/am3BpP5ZOoNwFSePXs2/vjjj3H16tVx+vTp8eeff4579+6NjY2NcefOnXHt2rWptwgrd8yFePCvd+/ejfX19fHXX3+NnZ2dqbcDK+fPR/A/Tpw4Mb766quxt7c39VZgEv58xJH35s2b8fbt2/Hq1avxyy+/jO3t7XHlypWptwWTEAWOvBs3bow7d+6MMcY4fvz4uHjx4rh169bEu4JpOFPgyNvZ2RnPnz8fL168GHfv3h0nT54ct2/fHl9++eXUW4OVEwV4z7fffjv29vbGr7/+Oo4dOzb1dmClHDTDey5dujQePnw4njx5MvVWYOVEAd7z9u3bMcYYr169mngnsHqiwJH122+//Wft77//Hj/++OP49NNPxzfffDPBrmBapo84sr777rvx+vXrce7cuXHmzJnx8uXL8dNPP42dnZ3x/fffj88++2zqLcLKOWjmyPr555/HDz/8MB49ejR+//338fnnn4/19fWxubk5NjY2pt4eTEIUAIgzBQAiCgBEFACIKAAQUQAgogBAFv7nNReDLW53d3fm+vXr12eunz9/fuHXTmFra2vm+vb29lLrwLQW+Q8E3xQAiCgAEFEAIKIAQEQBgCx8Id4U00ezpnLGmD3d8/Tp0w+9nf+Yt7+Pbfrm7NmzM9fnTUgdpskp4F+mjwBYiigAEFEAIKIAQEQBgCw8fXThwoWZ68tMCN28eXOJrR2u6aNZEzjz7jg66vdEbW5uzlyfNZU1xdQYHFWmjwBYiigAEFEAIKIAQBY+aF5bW1vqjR0gHl0f8vqPeYfYs37fPrbrRuD/5aAZgKWIAgARBQAiCgBEFADIwtNH8x6csuzVFQBMw/QRAEsRBQAiCgBEFACIKACQhaePDuLBMbMeVDOGe5JYLb+HHFWmjwBYiigAEFEAIKIAQEQBgHyyyh827+lt8+5Vmrc+a3pk3tO+Dupuplk/8yD2Pcbhn3rZ2tpa6vW7u7sz1w/L09GWfTLcYf984CD5pgBARAGAiAIAEQUAstJrLuYdQM4z70Bw1sHfFIe4837mPMseTM8y79B3mfc4KPM+nwcPHsxcP4g9zvs8Z/1uzXvt5ubmzHUHzXzsXHMBwFJEAYCIAgARBQAiCgBkpdNHy14vcFjemw9n2amxea+fNdm07PUcpoz42Jk+AmApogBARAGAiAIAEQUA8sEesrPsvUCH5b1Z3LzPYZkHHs2b+Jl3f9K8abIp7n6Cj5FvCgBEFACIKAAQUQAgogBAPtjdR7MmU+ZNjqytrS38HmPMnjQxfXJ4HMRT8NxlBQfP3UcALEUUAIgoABBRACCiAEBW+uQ1AKZj+giApYgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAfLLoC/f39z/kPgA4BHxTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAg/wCadHsoVWOnOgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drawing = train_dataset.__getitem__(300000)\n",
    "\n",
    "print(drawing)\n",
    "import matplotlib.pyplot as plt\n",
    "print(drawing[0].shape)\n",
    "\n",
    "# Visualize the image\n",
    "plt.imshow(drawing[0].squeeze(), cmap='gray')  # Squeeze to remove the single channel dimension\n",
    "plt.axis('off')  # Hide axis for better visualization\n",
    "plt.title(drawing[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example input data (simplified vector data from the NDJSON)\n",
    "drawings = [\n",
    "    [[[4,18,29,63,93,120,146,169,186,218,244,234,186,154,128,86,44,14,0],[7,51,66,90,101,106,106,101,93,67,22,23,49,58,59,53,26,16,6]],[[10,27,42,78,135,162,212,230,244],[15,39,53,67,80,74,48,35,20]],[[9,2,16,22,23,20],[18,3,0,3,8,20]],[[229,244,254,252,241],[23,17,18,22,30]],[[52,52],[52,52]],[[52,50],[52,52]],[[59,43],[69,61]]],\n",
    "    [[[223,226,227,233,254,255,248,227],[35,25,2,0,2,21,27,30]],[[234,235,244,246,249],[4,8,12,19,20]],[[232,208,168,135,98,37],[28,51,104,138,164,196]],[[255,255,245,194,182,172,115,26,18,8],[25,61,106,199,213,217,219,235,235,230]],[[12,1,0,0,10,22,29,28,16,11],[193,191,194,206,219,218,206,199,194,197]],[[41,29,12],[232,228,217]],[[7,7,12,14,14,18,19,23,27,29,37],[186,199,190,193,209,200,204,204,194,203,192]],[[230,230,235,238,241,245,245],[8,15,8,17,16,4,11]],[[251,251,241,235,211,191,121,59,31],[26,69,113,126,155,168,200,215,215]],[[214,201,177,153,120,108],[72,102,131,152,167,175]]], \n",
    "    [[[2,0,4,24,41,94,160,186,189,177,151,134,94,71,30],[0,38,79,138,162,207,247,255,251,238,218,200,139,98,45]]]\n",
    "]\n",
    "\n",
    "# Reconstruct the image from the simplified drawing\n",
    "index = 0\n",
    "for drawing in drawings:\n",
    "    reconstructed_image = train_dataset.drawing_to_image(drawing)\n",
    "    index+=1\n",
    "    # Display the image\n",
    "    plt.imshow(reconstructed_image, cmap='gray')\n",
    "    plt.title(index)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just a resizing test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming `drawing` is the tuple as described\u001b[39;00m\n\u001b[1;32m      8\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m drawing[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Image tensor of shape (1, H, W)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mdrawing\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m         \u001b[38;5;66;03m# The label (in this case, 3)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Assuming `image_tensor` is the tensor to resize, shape: (1, H, W)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Convert the tensor to a PIL image\u001b[39;00m\n\u001b[1;32m     13\u001b[0m image_numpy \u001b[38;5;241m=\u001b[39m image_tensor\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Remove channel and convert to NumPy\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming `drawing` is the tuple as described\n",
    "image_tensor = drawing[0]  # Image tensor of shape (1, H, W)\n",
    "label = drawing[1]         # The label (in this case, 3)\n",
    "\n",
    "# Assuming `image_tensor` is the tensor to resize, shape: (1, H, W)\n",
    "# Convert the tensor to a PIL image\n",
    "image_numpy = image_tensor.squeeze().cpu().numpy()  # Remove channel and convert to NumPy\n",
    "image_pil = Image.fromarray((image_numpy * 255).astype(np.uint8))  # Convert to uint8 image\n",
    "\n",
    "# Resize the image using PIL\n",
    "resized_image_pil = image_pil.resize((64, 64), Image.LANCZOS)\n",
    "\n",
    "# Convert back to NumPy or tensor if needed\n",
    "resized_image_numpy = np.array(resized_image_pil) / 255.0  # Normalize back if needed\n",
    "\n",
    "# Visualize the original and resized image\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original image\n",
    "ax[0].imshow(image_numpy, cmap='gray')\n",
    "ax[0].set_title(\"Original Image (256x256)\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "# Resized image\n",
    "ax[1].imshow(resized_image_numpy, cmap='gray')\n",
    "ax[1].set_title(\"Resized Image (64x64)\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):  # Number of epochs\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Testing loop\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16*16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model structure SimpleCNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=8192, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1, Loss: 0.4110197599155996\n",
      "Epoch 2, Loss: 0.31496348395428075\n",
      "Epoch 3, Loss: 0.2823221774107087\n",
      "Epoch 4, Loss: 0.259060027471739\n",
      "Epoch 5, Loss: 0.23983934732956932\n",
      "Epoch 6, Loss: 0.22329707152937295\n",
      "Epoch 7, Loss: 0.2094609522234722\n",
      "Epoch 8, Loss: 0.19705468979870466\n",
      "Epoch 9, Loss: 0.18663024016773255\n",
      "Epoch 10, Loss: 0.1772725153415165\n",
      "Accuracy: 89.34%\n",
      "Execution time: 8612.358917 seconds\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"model structure\", model)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(model, optimizer, criterion)\n",
    "test_model(model)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to best_model2.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "model_file_path = \"best_model2.pth\"\n",
    "torch.save(model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightModel structure EfficientCNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=8192, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1, Loss: 0.4318207372455258\n",
      "Epoch 2, Loss: 0.338471194588679\n",
      "Epoch 3, Loss: 0.30951562185115966\n",
      "Epoch 4, Loss: 0.29060624745590086\n",
      "Epoch 5, Loss: 0.27626049889245025\n",
      "Epoch 6, Loss: 0.2647074138012333\n",
      "Epoch 7, Loss: 0.25459925987843174\n",
      "Epoch 8, Loss: 0.24577768132687794\n",
      "Epoch 9, Loss: 0.23832450930444152\n",
      "Epoch 10, Loss: 0.231188982910864\n",
      "Accuracy: 89.33%\n",
      "Execution time: 8956.930888 seconds\n"
     ]
    }
   ],
   "source": [
    "class EfficientCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EfficientCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # 1 input channel, 16 filters\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Reduce size by half\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16 * 16, 64),  # Fewer neurons in FC layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "lightModel = EfficientCNN(num_classes).to(device)\n",
    "optimizer = optim.Adam(lightModel.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"lightModel structure\", lightModel)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(lightModel, optimizer, criterion)\n",
    "test_model(lightModel)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightModel structure EnhancedCNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=4096, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1, Loss: 0.41568543019304716\n",
      "Epoch 2, Loss: 0.31373000416233426\n",
      "Epoch 3, Loss: 0.2946580418789399\n",
      "Epoch 4, Loss: 0.28409182778481223\n",
      "Epoch 5, Loss: 0.27558986535428304\n",
      "Epoch 6, Loss: 0.2701137036165174\n",
      "Epoch 7, Loss: 0.26667471439594254\n",
      "Epoch 8, Loss: 0.26374345910756386\n",
      "Epoch 9, Loss: 0.26139851106363676\n",
      "Epoch 10, Loss: 0.25988344687222026\n",
      "Accuracy: 93.13%\n",
      "Execution time: 8102.731573 seconds\n"
     ]
    }
   ],
   "source": [
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),  # Normalize feature maps for faster convergence\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Add a third convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 256),  # More neurons for higher capacity\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Add dropout for regularization\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "deepModel = EnhancedCNN(num_classes).to(device)\n",
    "optimizer = optim.Adam(deepModel.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"lightModel structure\", deepModel)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(deepModel, optimizer, criterion)\n",
    "test_model(deepModel)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to deepest_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "model_file_path = \"deepest_model.pth\"\n",
    "torch.save(deepModel.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two deeper models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_model_with_scheduler(model, optimizer, scheduler, criterion, num_epochs=10):\n",
    "    print(num_epochs)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning Rate: {param_group['lr']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetLikeCNN structure:\n",
      "ResNetLikeCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (shortcut): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(output_size=(8, 8))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/10], Loss: 0.5782\n",
      "Epoch [2/10], Loss: 0.4584\n",
      "Epoch [3/10], Loss: 0.4262\n",
      "Epoch [4/10], Loss: 0.4053\n",
      "Epoch [5/10], Loss: 0.3914\n",
      "Epoch [6/10], Loss: 0.3814\n",
      "Epoch [7/10], Loss: 0.3732\n",
      "Epoch [8/10], Loss: 0.3651\n",
      "Epoch [9/10], Loss: 0.3596\n",
      "Epoch [10/10], Loss: 0.3542\n",
      "Accuracy: 90.80%\n",
      "Execution time: 14662.274112 seconds\n",
      "Model saved to deeper1_model_dynamic.pth\n"
     ]
    }
   ],
   "source": [
    "class ResNetLikeCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetLikeCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Shortcut connection to match the channel dimensions\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=1),  # Match input to output channels\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Ensure consistent spatial dimensions\n",
    "        self.fc = None  # Placeholder, will be dynamically initialized\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))  # First convolution\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))  # Second convolution\n",
    "        residual = self.shortcut(x)              # Adjust dimensions for residual\n",
    "        x = torch.relu(self.bn3(self.conv3(x)) + residual)  # Add residual to output\n",
    "        x = self.pool(x)  # Downsample to fixed size\n",
    "        x = torch.flatten(x, 1)  # Flatten before fully connected layers\n",
    "        x = self.fc(x)  # Fully connected layers\n",
    "        return x\n",
    "\n",
    "    def initialize_fc(self, input_shape):\n",
    "        # Dynamically calculate the input size of the flattened feature map\n",
    "        dummy_input = torch.zeros(input_shape)  # Create a dummy tensor with input shape\n",
    "        conv_output = self.conv1(dummy_input)  # Pass through layers to determine flattened size\n",
    "        conv_output = torch.relu(self.bn1(conv_output))\n",
    "        conv_output = torch.relu(self.bn2(self.conv2(conv_output)))\n",
    "        residual = self.shortcut(conv_output)\n",
    "        conv_output = torch.relu(self.bn3(self.conv3(conv_output)) + residual)\n",
    "        conv_output = self.pool(conv_output)\n",
    "        flattened_size = torch.flatten(conv_output, 1).shape[1]  # Calculate flattened size\n",
    "        \n",
    "        # Define the fully connected layers dynamically\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "# Define number of classes and input shape\n",
    "input_shape = (1, 1, 64, 64)  # Batch size of 1, 1 channel, 64x64 image size\n",
    "\n",
    "# Instantiate the model and initialize fully connected layers\n",
    "deeper1Model = ResNetLikeCNN(num_classes=num_classes)\n",
    "deeper1Model.initialize_fc(input_shape)\n",
    "\n",
    "# Move model to the device\n",
    "deeper1Model = deeper1Model.to(device)\n",
    "\n",
    "# Define optimizer, scheduler, and criterion\n",
    "optimizer = optim.Adam(deeper1Model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print model structure\n",
    "print(\"ResNetLikeCNN structure:\")\n",
    "print(deeper1Model)\n",
    "\n",
    "# Training and testing\n",
    "start_time = time.time()\n",
    "train_model(deeper1Model, optimizer, scheduler, criterion, num_epochs)\n",
    "test_model(deeper1Model)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "# Save the trained model\n",
    "model_file_path = \"deeper1_model_dynamic.pth\"\n",
    "torch.save(deeper1Model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeeperCNN2 structure:\n",
      "DeeperCNN2(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc2): Linear(in_features=512, out_features=12, bias=True)\n",
      "  (fc1): Linear(in_features=8192, out_features=512, bias=True)\n",
      ")\n",
      "Epoch [1/10], Loss: 0.4659\n",
      "Epoch [2/10], Loss: 0.3979\n",
      "Epoch [3/10], Loss: 0.3839\n",
      "Epoch [4/10], Loss: 0.3762\n",
      "Epoch [5/10], Loss: 0.3717\n",
      "Epoch [6/10], Loss: 0.3676\n",
      "Epoch [7/10], Loss: 0.3643\n",
      "Epoch [8/10], Loss: 0.3627\n",
      "Epoch [9/10], Loss: 0.3605\n",
      "Epoch [10/10], Loss: 0.3589\n",
      "Accuracy: 88.95%\n",
      "Execution time: 9541.829543 seconds\n",
      "Model saved to deeper2_model_dynamic.pth\n"
     ]
    }
   ],
   "source": [
    "class DeeperCNN2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeeperCNN2, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc1 = None  # Placeholder, will be initialized dynamically\n",
    "        self.fc2 = nn.Linear(512, num_classes)  # Output layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)  # Pass through convolutional layers\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch size\n",
    "        x = self.fc1(x)  # First fully connected layer\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "    def initialize_fc(self, input_shape):\n",
    "        # Dynamically calculate the input size of the flattened feature map\n",
    "        dummy_input = torch.zeros(input_shape)  # Create a dummy tensor with input shape\n",
    "        conv_output = self.conv_layers(dummy_input)  # Pass through conv layers\n",
    "        flattened_size = torch.flatten(conv_output, 1).shape[1]  # Calculate flattened size\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)  # Initialize fc1 with calculated size\n",
    "\n",
    "\n",
    "input_shape = (1, 1, 64, 64)  # Batch size of 1, 1 channel, 64x64 image size\n",
    "\n",
    "# Instantiate the model and initialize fully connected layer\n",
    "deeper2Model = DeeperCNN2(num_classes=num_classes)\n",
    "deeper2Model.initialize_fc(input_shape)\n",
    "\n",
    "# Move model to the device\n",
    "deeper2Model = deeper2Model.to(device)\n",
    "\n",
    "# Define optimizer, scheduler, and criterion\n",
    "optimizer = optim.Adam(deeper2Model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print model structure\n",
    "print(\"DeeperCNN2 structure:\")\n",
    "print(deeper2Model)\n",
    "\n",
    "# Training and testing\n",
    "start_time = time.time()\n",
    "train_model(deeper2Model, optimizer, scheduler, criterion, num_epochs)\n",
    "test_model(deeper2Model)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "# Save the trained model\n",
    "model_file_path = \"deeper2_model_dynamic.pth\"\n",
    "torch.save(deeper2Model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing = [[[232,232,234,234,234,234,234,234,235,235,235,235,235,235,235,235,235,234,234,234,234,234,234,234,234,234,234,234,235,235,235,235,235,235,235,235,235,235,235,235,235,235,234],[393,395,395,396,397,398,400,401,401,402,403,405,406,407,408,410,411,411,412,413,415,416,418,420,421,422,423,425,425,426,427,428,430,431,432,433,435,436,437,438,440,441,441],[0,89,96,104,120,129,137,145,160,168,176,184,192,200,208,216,224,245,253,264,272,295,320,336,353,385,424,449,470,480,504,520,536,553,562,576,584,603,629,708,729,763,777]],[[248,248,249,249,249,249,249,249,250,250,250,250,250,250,250,250,250,250,250,250,250,250,250,250,250,250,249,249,249,250,250,250,250,250,250,250,250,251,251,251,251],[398,400,400,401,402,403,405,406,406,407,408,410,411,412,413,415,416,417,418,420,421,422,423,425,426,427,427,428,430,430,431,432,433,435,436,437,438,438,440,441,442],[1433,1545,1552,1561,1569,1577,1586,1600,1608,1616,1624,1644,1656,1677,1688,1711,1719,1745,1755,1778,1803,1811,1836,1849,1882,1905,1913,1936,1962,1986,2002,2027,2052,2084,2116,2187,2211,2219,2235,2261,2393]],[[251,252,254,255,255,256,256,257,259,259,258,258,257,256,256,256,254,254,253,253,252,251,251,249,249,248,248,247,247,246,244,244,243,242,241,241,239,238,238,237,237,236,236,234,234,233,232,232,231,229,229,228,227,227,226,226,224,223,223,224,225,226,226,227,229,229,230,231,232,232,234,235],[442,442,442,442,443,443,445,445,445,446,446,447,447,447,448,450,450,451,452,453,455,455,456,456,457,457,458,458,460,460,460,461,461,461,461,460,460,460,459,459,458,458,457,457,455,455,455,454,453,453,452,452,452,450,450,449,449,449,448,448,447,447,445,445,445,444,444,443,443,442,442,442],[2608,2693,2710,2736,2744,2752,2760,2769,2803,2810,3025,3065,3084,3112,3119,3161,3168,3247,3258,3274,3283,3322,3346,3449,3556,3584,3593,3618,3668,3683,3764,3796,3819,3881,3920,3928,3936,3959,3967,3979,3986,3995,4002,4010,4017,4025,4041,4050,4065,4092,4105,4125,4154,4178,4200,4209,4225,4273,4481,4488,4513,4545,4566,4576,4586,4592,4617,4649,4659,4666,4737,4833]],[[252,252,252,252,252,252,252,251,251],[402,400,399,398,397,395,394,394,393],[5699,5837,5866,5906,5924,5954,5986,6136,6180]],[[251,252,254,255,256,257,259,260,261,261,262,264,265,266,267,269,270,270,271,272,274,274,275,276,277,279,279,280,281,281,281,282,282,284,284,283,282,282,281,279,278,277,277,276],[393,393,393,393,393,393,393,393,393,392,392,392,392,392,392,392,392,390,390,389,389,388,388,385,385,384,383,383,382,380,379,379,378,378,377,377,377,375,375,374,374,374,373,373],[6776,6904,6945,6964,6973,6985,6993,7001,7023,7059,7065,7073,7081,7106,7154,7161,7181,7189,7197,7250,7257,7280,7289,7297,7305,7314,7328,7336,7355,7427,7434,7441,7449,7457,7481,7764,7772,7787,7797,7822,7847,7858,7866,7914]],[[276,276,276,276,274,273,273,272,272,271,271,272,274,274,275,275,274,273,272,271,269,269,268,268,268,268,267,266,266,266,264,264,264,263,263,262,262,261,259,258,258,257,256,256,254,253,252,251,249,248,247,246,244,243,243,242,241,239,238,237,232,227,226,224,223,222,221,219,218,217,217,216,214,213,213,212,212,211,208,208,207,207,207,209,209,210,210,209,209,208,207,206,206,207,209,209,210,211,211,211,211,211,211,212,212,214,215,216,216,217,219,219,220,221,221,222,222,224,224,225,226,227,229,230,231,232,234,235],[373,372,370,369,369,369,368,368,367,367,365,364,364,363,363,362,360,360,359,359,359,358,358,360,361,362,362,362,363,365,365,366,367,367,368,368,370,370,370,371,372,372,372,373,373,373,373,373,375,375,375,375,375,374,375,375,375,376,376,376,376,376,376,376,376,376,376,376,375,375,374,374,373,373,372,372,370,370,370,371,371,372,373,373,375,375,376,376,377,377,377,377,378,378,378,380,380,380,381,382,383,385,386,386,387,387,387,387,388,388,388,390,390,390,391,391,392,392,393,393,393,393,393,393,393,393,393,393],[8768,9048,9137,9448,9497,9537,9545,9560,9568,9586,9595,9977,9988,10105,10112,10120,10417,10473,10479,10544,10576,10587,10977,10985,11000,11057,11064,11088,11099,11119,11127,11163,11172,11179,11202,11218,11244,11252,11265,11273,11298,11306,11327,11335,11344,11352,11364,11396,11419,11427,11482,11510,11521,11530,11537,11546,11553,11561,11569,11577,11585,11593,11601,11610,11618,11666,11714,11754,11810,11818,11850,11876,11890,11897,11904,11912,11920,11928,12026,12761,12833,12900,13216,13273,13337,13344,13366,13588,13617,13756,13848,13864,14016,14073,14088,14112,14119,14141,14208,14377,14409,14472,14521,14745,14765,14773,14825,14848,14856,14872,14913,14953,14973,15016,15051,15082,15090,15130,15145,15173,15211,15234,15293,15315,15340,15379,15445,15465]],[[232,234,235,236,237,239,240,241,242,244,245,246,247,249,250,251,252],[397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397],[16944,17136,17153,17178,17191,17248,17270,17295,17344,17376,17409,17428,17475,17495,17506,17581,17674]],[[234,234,234,234,234,234,234,234,234,234,234,234,234,234,234,234,234,234,234],[377,375,374,373,372,370,369,368,367,365,364,363,362,360,359,358,357,355,354],[20138,20306,20339,20368,20378,20403,20427,20451,20483,20506,20534,20547,20576,20603,20626,20635,20642,20657,20676]],[[252,251,251,251,251,249,249,249,249,249,249,249,249,248,248,248,248,249,249,249,249,249],[376,375,374,373,372,372,370,369,368,367,365,364,363,363,362,360,359,359,358,357,355,354],[22167,22267,22274,22299,22314,22330,22338,22358,22370,22399,22442,22465,22522,22541,22553,22586,22661,22682,22715,22738,22787,22808]],[[234,234,233,233,232,231,231,229,228,228,227,227,226,224,224,224,225,225,226,226,226,226,227,229,229,230,230,231,231],[355,354,354,353,353,352,350,350,350,349,349,348,348,348,347,345,344,343,342,340,339,338,337,337,335,335,334,334,333],[23498,23619,23648,23656,23665,23689,23722,23756,23764,23777,23798,23834,23868,23889,24072,24097,24122,24135,24160,24200,24248,24272,24570,24577,24618,24666,24688,24697,24713]],[[252,252,252,254,254,255,255,256,257,257,259,258,258,258,258,258,258,258,258,258,257,257,256,254,253,253,252],[355,354,353,353,352,352,350,350,350,349,349,349,348,347,345,344,343,342,340,339,339,338,338,338,338,337,337],[25599,25800,25807,25832,25854,25864,25921,25928,25953,25962,25991,26291,26299,26323,26347,26371,26395,26435,26454,26479,26744,26761,26776,26824,26865,26963,26987]],[[232,232,232,232,232,232,232,232,232,232,232,232,232,232,232,232,232,232,232,232,232,232,232,232,231,231,231,231,231,231,231,231,231,229,229,229,229,230,230,230,230,230,230,230,230,230,230,230,230,230,230,230,230,231,231,231,231,231,231,231,232,232,232,232,232,232,232,234,234,234,235,235,236,236,236,237,237,239,238,239,240,240,241],[334,333,332,330,329,328,327,325,324,323,322,320,319,318,317,315,314,313,312,310,309,308,307,305,305,304,303,302,297,295,294,293,292,292,287,285,284,284,283,282,280,279,278,277,275,274,273,272,270,269,268,267,265,265,264,263,262,260,259,258,258,257,255,254,253,252,250,250,249,248,248,247,247,245,244,244,243,243,242,242,242,240,240],[27727,27872,27896,27936,27960,27985,27999,28024,28031,28088,28136,28167,28188,28210,28244,28258,28268,28277,28285,28347,28368,28376,28417,28428,28435,28443,28451,28460,28468,28476,28510,28518,28577,28591,28599,28624,28631,28639,28647,28680,28687,28694,28717,28726,28776,28791,28817,28826,28866,28922,28955,28986,29011,29018,29061,29085,29115,29143,29154,29244,29251,29259,29284,29309,29322,29351,29379,29451,29466,29515,29543,29551,29563,29584,29620,29634,29754,29761,29769,29777,29801,29849,29883]],[[256,256,256,256,256,256,256,256,256,254,254,254,254,254,253,253,253,253,253,253,253,253,253,252,252,252,252,252,252,252,252,252,252,252,252,252,252,252,252,251,251,251,251,251,249,249,249,249,249,249,249,249,249,249,249,249,249,249,249,249,249,249,248,248,248,248,248,248,248,248,248,247,247,247,247,246,246,246,246,246,246,246,246,244,243,243,242,241,239],[340,339,338,337,335,334,333,332,330,330,329,328,327,325,325,324,323,322,320,319,318,317,315,314,313,312,310,309,308,307,305,304,303,302,300,299,298,297,295,295,294,293,292,290,289,288,287,285,284,283,282,280,279,278,277,275,274,273,272,270,269,268,268,267,265,264,263,262,260,259,258,258,257,255,254,254,253,252,250,249,248,247,245,245,245,244,244,244,244],[31199,31256,31283,31323,31347,31366,31391,31399,31424,31449,31474,31482,31498,31515,31524,31532,31546,31565,31590,31615,31625,31648,31657,31682,31691,31704,31723,31748,31759,31781,31806,31815,31840,31872,31896,31914,31927,31952,31976,31983,32006,32015,32040,32056,32081,32095,32120,32127,32151,32173,32186,32207,32235,32266,32291,32315,32330,32416,32439,32448,32464,32496,32528,32535,32556,32581,32608,32631,32655,32680,32712,32739,32747,32760,32781,32792,32800,32822,32830,32839,32847,32855,32864,33000,33022,33047,33055,33112,33123]],[[108],[115],[43729]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(drawing))\n\u001b[1;32m      3\u001b[0m prediction_dataset \u001b[38;5;241m=\u001b[39m QuickDrawDataset(drawing, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m, resize_to\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m drawing \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(drawing[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Squeeze to remove the single channel dimension\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Hide axis for better visualization\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 24\u001b[0m, in \u001b[0;36mQuickDrawDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Convert the drawing format to image\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     drawing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrawings\u001b[38;5;241m.\u001b[39miloc[idx] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrawings, pd\u001b[38;5;241m.\u001b[39mSeries) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrawings[idx]\n\u001b[0;32m---> 24\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrawing_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrawing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39miloc[idx] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels, pd\u001b[38;5;241m.\u001b[39mSeries) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Convert the image to a tensor and add batch dimension\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 42\u001b[0m, in \u001b[0;36mQuickDrawDataset.drawing_to_image\u001b[0;34m(self, drawing)\u001b[0m\n\u001b[1;32m     39\u001b[0m y_coords \u001b[38;5;241m=\u001b[39m stroke[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# For each stroke, draw lines between consecutive points\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_coords\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     43\u001b[0m     x1, y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x_coords[i]), \u001b[38;5;28mint\u001b[39m(y_coords[i])\n\u001b[1;32m     44\u001b[0m     x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x_coords[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mint\u001b[39m(y_coords[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "print(len(drawing))\n",
    "prediction_dataset = QuickDrawDataset(drawing, \"unknown\", resize_to=(64, 64))\n",
    "\n",
    "drawing = prediction_dataset.__getitem__(0)\n",
    "\n",
    "plt.imshow(drawing[0].squeeze(), cmap='gray')  # Squeeze to remove the single channel dimension\n",
    "plt.axis('off')  # Hide axis for better visualization\n",
    "plt.title(drawing[1])\n",
    "plt.show()\n",
    "\n",
    "# drawing = train_dataset.__getitem__(300000)\n",
    "# drawing = drawing[0].squeeze()\n",
    "# reconstructed_image = train_dataset.drawing_to_image(drawing)\n",
    "\n",
    "# reconstructed_image = train_dataset.drawing_to_image(drawing)\n",
    "plt.imshow(reconstructed_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load(\"best_model2.pth\", map_location=torch.device('cpu')))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Prepare the input image\n",
    "# Assuming reconstructed_image is a PIL Image\n",
    "transform = ToTensor()  # Convert PIL image to tensor\n",
    "input_image = transform(reconstructed_image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():  # No need to compute gradients for inference\n",
    "    output = model(input_image)\n",
    "    predicted_class = word_class_mapping[torch.argmax(output, dim=1).item()]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
