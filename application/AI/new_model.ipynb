{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/bin/python\n",
      "Requirement already satisfied: pandas in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: ndjson in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (0.3.1)\n",
      "Requirement already satisfied: google-cloud-storage in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (2.37.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (2.24.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (1.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (5.29.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.25.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: six>=1.5 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.12.14)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!pip install pandas ndjson google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import pandas as pd\n",
    "from download import download_data_and_parse_it\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 12\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "\n",
    "#import os\n",
    "#import json\n",
    "#from google.cloud import storage\n",
    "\n",
    "# Configurer la variable d'environnement\n",
    "#os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./copper-gear-448109-p4-3afc6f7125c4.json\"\n",
    "\n",
    "# Vérifier la variable d'environnement\n",
    "#credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "#if credentials_path:\n",
    "#    credentials_path = os.path.abspath(credentials_path)  # Obtenir le chemin absolu\n",
    "#    print(f\"Path to credentials: {credentials_path}\")\n",
    "    \n",
    "#    if os.path.exists(credentials_path):\n",
    "        # Charger et afficher le contenu du fichier JSON\n",
    "#        with open(credentials_path, 'r') as f:\n",
    "#            credentials_content = json.load(f)\n",
    "#            print(\"Credentials file content:\")\n",
    "#            print(json.dumps(credentials_content, indent=4))\n",
    "#    else:\n",
    "#        print(\"Le fichier spécifié n'existe pas.\")\n",
    "#else:\n",
    "#    print(\"La variable d'environnement GOOGLE_APPLICATION_CREDENTIALS n'est pas définie.\")\n",
    "\n",
    "# Lister les buckets disponibles\n",
    "#try:\n",
    "#    client = storage.Client()\n",
    "#    buckets = list(client.list_buckets())\n",
    "#    print(f\"Buckets disponibles : {[bucket.name for bucket in buckets]}\")\n",
    "#except Exception as e:\n",
    "#    print(f\"Erreur lors de la connexion à Google Cloud Storage : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier 'full/simplified/star.ndjson' existe dans le bucket 'quickdraw_dataset'.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "bucket_name = \"quickdraw_dataset\"\n",
    "file_path = \"full/simplified/star.ndjson\"\n",
    "\n",
    "try:\n",
    "    client = storage.Client.create_anonymous_client()  # Client anonyme pour bucket public\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_path)\n",
    "\n",
    "    if blob.exists():\n",
    "        print(f\"Le fichier '{file_path}' existe dans le bucket '{bucket_name}'.\")\n",
    "    else:\n",
    "        print(f\"Le fichier '{file_path}' n'existe pas dans le bucket '{bucket_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'accès au bucket : {e}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement du fichier : full/simplified/star.ndjson\n",
      "Données brutes du fichier full/simplified/star.ndjson :\n",
      "{\"word\":\"star\",\"countrycode\":\"US\",\"timestamp\":\"2017-03-07 16:39:23.36509 UTC\",\"recognized\":true,\"key_id\":\"5260413706960896\",\"drawing\":[[[67,37,30,44,70,105,116,184,192,194,161,190,237,230,185,155,145,110,102,98,96,78,76,70,0,18,35,48,60],[160,220,243,240,226,200,189,251,255,249,160,145,106,103,103,95,72,9,0,5,35,110,104,101,91,128,154,165,166]]]}\n",
      "{\"word\":\"star\",\"countrycode\":\"US\",\"timestamp\":\"2017-03-15 15:13:18.18952 UTC\",\"recognized\":true,\"key_id\":\"4853913289228288\",\"drawing\":[[[104,106,115,12\n",
      "Téléchargement du fichier : full/simplified/apple.ndjson\n",
      "Données brutes du fichier full/simplified/apple.ndjson :\n",
      "{\"word\":\"apple\",\"countrycode\":\"US\",\"timestamp\":\"2017-03-10 22:17:57.57466 UTC\",\"recognized\":false,\"key_id\":\"6420579601088512\",\"drawing\":[[[255,255],[0,0]],[[255,255],[0,0]],[[255,255],[0,0]],[[255,254],[0,1]],[[131,124,114,69,37,10,0,0,5,16,31,50,68,86,101,115,126,135,137,135,122,106],[50,39,39,59,89,127,172,194,215,233,244,249,249,241,225,203,174,143,114,88,65,45]],[[84,77,81,88,99,122,138,161,180],[97,85,52,34,18,4,1,2,12]]]}\n",
      "{\"word\":\"apple\",\"countrycode\":\"RU\",\"timestamp\":\"2017-03-08 06:29:44.\n",
      "size of datasets array : 2\n"
     ]
    }
   ],
   "source": [
    "import ndjson\n",
    "\n",
    "bucket_name = \"quickdraw_dataset\"\n",
    "\n",
    "datasets = [\"star\", \"apple\"]\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    file_path = f\"full/simplified/{dataset}.ndjson\"\n",
    "    \n",
    "    print(f\"Téléchargement du fichier : {file_path}\")\n",
    "    try:\n",
    "        dataset_df = download_data_and_parse_it(bucket_name,file_path)\n",
    "        all_dfs.append(dataset_df)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erreur : {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur inattendue : {e}\")\n",
    "\n",
    "# Combine all datasets\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Assign class labels using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['word'])\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)  # 60% train, 40% temp\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # Split temp into 20% val, 20% test\n",
    "\n",
    "print(\"size of datasets array :\", len(datasets))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download import download_data_and_parse_it\n",
    "# Load the JSON file into a DataFrame\n",
    "datasets = [\n",
    "        \"star\", \"sword\", \"tent\", \"apple\", \"banana\", \"cat\", \n",
    "        \"dog\", \"car\", \"house\", \"tree\", \"guitar\", \"bicycle\"\n",
    "    ]\n",
    "    \n",
    "all_dfs = []  # List to store all DataFrames\n",
    "\n",
    "for dataset in datasets:\n",
    "    file_path = f\"full/simplified/{dataset}.ndjson\"\n",
    "    print(f\"Téléchargement du fichier : {file_path}\")\n",
    "    try:\n",
    "        dataset_df = download_data_and_parse_it(bucket_name, file_path)\n",
    "        all_dfs.append(dataset_df)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erreur : {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur inattendue : {e}\")\n",
    "\n",
    "\n",
    "# Combine all datasets\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Assign class labels using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['word'])\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)  # 60% train, 40% temp\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # Split temp into 20% val, 20% test\n",
    "\n",
    "print(\"size of datasets array :\", len(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Vector-To-Image Algorithm - Drawing-to-image changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, drawings, labels, resize_to=(64, 64)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            drawings (list or array): List of drawing data (tensor or numpy arrays).\n",
    "            labels (list or array): List of class labels corresponding to each drawing.\n",
    "            resize_to (tuple): Target size for resizing the image.\n",
    "        \"\"\"\n",
    "        self.drawings = drawings\n",
    "        self.labels = labels\n",
    "        self.resize_to = resize_to  # Tuple (width, height)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.drawings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the drawing format to image\n",
    "        drawing = self.drawings.iloc[idx] if isinstance(self.drawings, pd.Series) else self.drawings[idx]\n",
    "        image = self.drawing_to_image(drawing)\n",
    "        label = self.labels.iloc[idx] if isinstance(self.labels, pd.Series) else self.labels[idx]\n",
    "        \n",
    "        # Convert the image to a tensor and add batch dimension\n",
    "        image_tensor = torch.FloatTensor(image).unsqueeze(0)\n",
    "        return image_tensor, label\n",
    "\n",
    "    def drawing_to_image(self, drawing):\n",
    "        # Create a blank 256x256 image\n",
    "        img_size = 256\n",
    "        image = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "        \n",
    "        # Iterate through the strokes\n",
    "        for stroke in drawing:\n",
    "            x_coords = stroke[0]\n",
    "            y_coords = stroke[1]\n",
    "\n",
    "            # For each stroke, draw lines between consecutive points\n",
    "            for i in range(len(x_coords) - 1):\n",
    "                x1, y1 = int(x_coords[i]), int(y_coords[i])\n",
    "                x2, y2 = int(x_coords[i + 1]), int(y_coords[i + 1])\n",
    "\n",
    "                # Ensure that coordinates stay within bounds\n",
    "                x1 = max(0, min(x1, img_size - 1))\n",
    "                y1 = max(0, min(y1, img_size - 1))\n",
    "                x2 = max(0, min(x2, img_size - 1))\n",
    "                y2 = max(0, min(y2, img_size - 1))\n",
    "\n",
    "                # Draw line on image (we simply put the endpoints as pixels here)\n",
    "                image[y1, x1] = 255\n",
    "                image[y2, x2] = 255\n",
    "        \n",
    "        # Resize the image to the desired size (64x64)\n",
    "        pil_image = Image.fromarray(image)  # Convert to PIL Image\n",
    "        pil_image = pil_image.resize(self.resize_to, Image.Resampling.LANCZOS)  # Resize image\n",
    "        \n",
    "        # Convert back to numpy array\n",
    "        resized_image = np.array(pil_image)\n",
    "        return resized_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create datasets\n",
    "# train_dataset = QuickDrawDataset(train_df['drawing'], train_df['class'])\n",
    "# val_dataset = QuickDrawDataset(val_df['drawing'], val_df['class'])\n",
    "# test_dataset = QuickDrawDataset(test_df['drawing'], test_df['class'])\n",
    "\n",
    "# # Create DataLoaders\n",
    "# from torch.utils.data import DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the datasets with resizing\n",
    "train_dataset = QuickDrawDataset(train_df['drawing'], train_df['class'], resize_to=(64, 64))\n",
    "val_dataset = QuickDrawDataset(val_df['drawing'], val_df['class'], resize_to=(64, 64))\n",
    "test_dataset = QuickDrawDataset(test_df['drawing'], test_df['class'], resize_to=(64, 64))\n",
    "\n",
    "# Create DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32  # Example batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to visualize some entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), np.int64(0))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAACs9JREFUeJzt3TGLY1Ufx/GTZwfcbYRYCLtgkxErEWGD8wZmEazNNjaCEN+AMFhYWQ2CYOmCiGA18R3MlKIou2gtTBAEF2wCNs4Ucq38oebkMdfZm2SSz6f87yWeZ5lnvhxy9txe0zRNAYBSyv/WvQAANocoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKLATru8vCxHR0flzp075datW+Xg4KCcnp6ue1mwNqLATnvzzTfLhx9+WN54443y0UcflRs3bpTXXnutfPnll+teGqxFz4V47Kpvv/22HBwclA8++KC88847pZRSLi4uyosvvlieffbZ8tVXX615hbB6dgrsrC+++KLcuHGjjMfjzG7evFneeuut8vXXX5effvppjauD9RAFdtZ3331XXnjhhfL000//bf7KK6+UUkr5/vvv17AqWC9RYGc9fvy43L59e27+5+znn39e9ZJg7USBnfXbb7+Vp556am5+8+bN/DnsGlFgZ926datcXl7OzS8uLvLnsGtEgZ11+/bt8vjx47n5n7M7d+6sekmwdqLAznr55ZfLDz/8UH799de/zb/55pv8OewaUWBnvf766+X3338vDx48yOzy8rJ8+umn5eDgoDz33HNrXB2sx966FwDrcnBwUEajUXn33XfLL7/8Up5//vny2WeflR9//LF88skn614erIV/0cxOu7i4KO+99175/PPPy2w2Ky+99FJ5//33y6uvvrrupcFaiAIA4TsFAEIUAAhRACBEAYAQBQBCFACIpf/xWq/X63IdsLHu3r279Pzs7Kz67KL3Pu/v7//3hUFLy/wLBDsFAEIUAAhRACBEAYAQBQDC1dnwLwaDQXVeO33013cz/JVTRlwXdgoAhCgAEKIAQIgCACEKAMTSr+N09xHA9ebuIwBaEQUAQhQACFEAIFxzwdZa9HKcRddWTCaTLpcD14KdAgAhCgCEKAAQogBAiAIA4fQRW2vRKaPDw8Pq3OkjsFMA4C9EAYAQBQBCFAAIUQAgvGQHYEd4yQ4ArYgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABB7V/2AwWBQnU+n06t+NCyt9nPoZxDas1MAIEQBgBAFAEIUAAhRACCWPn3UNE2rD+71eq0Xs0pHR0fV+Wg0qs6Hw2GXy2FJi067nZ+fz802/WcQNpGdAgAhCgCEKAAQogBAiAIA0WuWPFb0zDPPVOez2eyJLmhV+v1+q+ev6/9OgD8t8+veTgGAEAUAQhQACFEAIEQBgFj69JF7ZLpTu2/p8PCw+uzbb7/d9XKALeX0EQCtiAIAIQoAhCgAEEu/ZIfuTKfTudmjR49afcZ4PK7OF31O289n+3388cdzs7Ozs+qzk8nkyv+9hw8fVuf379+vzmv/P+HJs1MAIEQBgBAFAEIUAAhRACCcPtoAtZNAbU8H3b17tzpf9HIgp4/4p9pJoy5P/Cw6wbToZ7Z2HUwpi9foZ/y/sVMAIEQBgBAFAEIUAAhRACC8ZAdWYNHJGS9TWl7tbqZSur2fadt4yQ4ArYgCACEKAIQoABCiAEA4fQQrsOhuqsFgUJ07ObO8RX+Htb/zXf97dfoIgFZEAYAQBQBCFAAIXzQD19p4PK7Oa1807/r1Ib5oBqAVUQAgRAGAEAUAQhQAiM5OH/X7/aWfnc1mrT4bgPacPgKgFVEAIEQBgBAFAEIUAIi9rj64dh/JaDSqPjscDrtaBgAt2CkAEKIAQIgCACEKAIQoABDevAawI9x9BEArogBAiAIAIQoAhCgAEJ3dfQRXcXR0VJ0fHh5W5/fu3etyObAz7BQACFEAIEQBgBAFAMIXzazMYDCYm52cnFSfXfTF8aNHj57omoC/s1MAIEQBgBAFAEIUAAhRACCcPmJlZrPZ3GwymVSf7ff71fnp6Wl17iVQLKN2Aq6UxafghsNhl8vZSHYKAIQoABCiAECIAgAhCgCE00esTO300fHxcavPcMqIq6j9DJay+BTcLrJTACBEAYAQBQBCFAAIUQAgek3TNEs9uIZTH6PRqDqfTqdzM2/kAvj/lvl1b6cAQIgCACEKAIQoABCiAEBs9N1Hh4eH1fnZ2dnczOkjgKuzUwAgRAGAEAUAQhQAiI2+5gKAJ8c1FwC0IgoAhCgAEKIAQIgCALHR11ws0u/352az2WwNKwHYLnYKAIQoABCiAECIAgAhCgCEu48AdoS7jwBoRRQACFEAIEQBgBAFAOJa3n308OHDudn9+/erz06n066XA7A17BQACFEAIEQBgBAFAOJaftE8mUzmZl6yA3B1dgoAhCgAEKIAQIgCACEKAISX7ADsCC/ZAaAVUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQAiL11LwBgVfr9fnU+Ho+r8+Pj4y6Xs5HsFAAIUQAgRAGAEAUAotc0TbPUg71e12sB6NRgMKjOT05OqvPhcNjlclZumV/3dgoAhCgAEKIAQIgCACEKAITTRwA7wukjAFoRBQBCFAAIUQAgRAGA8JIdYGcsesnObDZb8Uo2l50CACEKAIQoABCiAECIAgDh7iOAHeHuIwBaEQUAQhQACFEAIFxzAWylwWAwNzs5Oak+OxwOu17OtWGnAECIAgAhCgCEKAAQogBAOH0EbKXai3Mmk8kaVnK92CkAEKIAQIgCACEKAIQoABBesgOwI7xkB4BWRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYDYW/cCttFgMKjOp9PpilcC0I6dAgAhCgCEKAAQogBAiAIA0WuaplnqwV6v67VsjUV/pfv7+9W5U0nAKizz695OAYAQBQBCFAAIUQAgRAGAcPdRB5zUAq4rOwUAQhQACFEAIEQBgPBFM9C52ounXO+ymewUAAhRACBEAYAQBQBCFAAIp4+AJ6Z2yqiUUs7Pz+dmroPZTHYKAIQoABCiAECIAgAhCgBEr2maZqkHnRQAuNaW+XVvpwBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEHvrXgDdapqmOt/f35+bTafTrpdDB/r9fqvnZ7NZRythG9gpABCiAECIAgAhCgCEKAAQTh9tudopo1KcNNom4/G4Oh+NRtX5cDjscjlcc3YKAIQoABCiAECIAgAhCgBEr1l0Oc4/H+z1ul4LAB1a5te9nQIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDE3roXwPbp9/vV+Xg8npsdHx+3+ozZbPbfFwb8KzsFAEIUAAhRACBEAYDoNU3TLPVgr9f1WtgSg8GgOj85OZmbDYfDJ/LfXPTFdI0vq9lVy/y6t1MAIEQBgBAFAEIUAAhRACCcPmIjnZ+fV+f37t2rzk9PT+dmDx48qD676GoN2HZOHwHQiigAEKIAQIgCACEKAMTSp48A2H52CgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMQf4CYONHhO0OcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drawing = train_dataset.__getitem__(300)\n",
    "\n",
    "print(drawing)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the image\n",
    "plt.imshow(drawing[0].squeeze(), cmap='gray')  # Squeeze to remove the single channel dimension\n",
    "plt.axis('off')  # Hide axis for better visualization\n",
    "plt.title(drawing[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example input data (simplified vector data from the NDJSON)\n",
    "drawings = [\n",
    "    [[[4,18,29,63,93,120,146,169,186,218,244,234,186,154,128,86,44,14,0],[7,51,66,90,101,106,106,101,93,67,22,23,49,58,59,53,26,16,6]],[[10,27,42,78,135,162,212,230,244],[15,39,53,67,80,74,48,35,20]],[[9,2,16,22,23,20],[18,3,0,3,8,20]],[[229,244,254,252,241],[23,17,18,22,30]],[[52,52],[52,52]],[[52,50],[52,52]],[[59,43],[69,61]]],\n",
    "    [[[223,226,227,233,254,255,248,227],[35,25,2,0,2,21,27,30]],[[234,235,244,246,249],[4,8,12,19,20]],[[232,208,168,135,98,37],[28,51,104,138,164,196]],[[255,255,245,194,182,172,115,26,18,8],[25,61,106,199,213,217,219,235,235,230]],[[12,1,0,0,10,22,29,28,16,11],[193,191,194,206,219,218,206,199,194,197]],[[41,29,12],[232,228,217]],[[7,7,12,14,14,18,19,23,27,29,37],[186,199,190,193,209,200,204,204,194,203,192]],[[230,230,235,238,241,245,245],[8,15,8,17,16,4,11]],[[251,251,241,235,211,191,121,59,31],[26,69,113,126,155,168,200,215,215]],[[214,201,177,153,120,108],[72,102,131,152,167,175]]], \n",
    "    [[[2,0,4,24,41,94,160,186,189,177,151,134,94,71,30],[0,38,79,138,162,207,247,255,251,238,218,200,139,98,45]]]\n",
    "]\n",
    "\n",
    "# Reconstruct the image from the simplified drawing\n",
    "index = 0\n",
    "for drawing in drawings:\n",
    "    reconstructed_image = train_dataset.drawing_to_image(drawing)\n",
    "    index+=1\n",
    "    # Display the image\n",
    "    plt.imshow(reconstructed_image, cmap='gray')\n",
    "    plt.title(index)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just a resizing test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `drawing` is the tuple as described\n",
    "image_tensor = drawing[0]  # Image tensor of shape (1, H, W)\n",
    "label = drawing[1]         # The label (in this case, 3)\n",
    "\n",
    "# Convert the image to 3D (H, W, C) for easier manipulation\n",
    "image_numpy = image_tensor.squeeze().cpu().numpy()\n",
    "\n",
    "# Resize the image to a smaller size using a transformation (e.g., 128x128)\n",
    "resize_transform = T.Resize((64, 64))  # Resize to 128x128\n",
    "resized_image_tensor = resize_transform(image_tensor.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "# Convert back to numpy array for visualization\n",
    "resized_image_numpy = resized_image_tensor.squeeze().cpu().numpy()\n",
    "\n",
    "# Visualize the original and resized image\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original image\n",
    "ax[0].imshow(image_numpy, cmap='gray')\n",
    "ax[0].set_title(\"Original Image (256x256)\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "# Resized image\n",
    "ax[1].imshow(resized_image_numpy, cmap='gray')\n",
    "ax[1].set_title(\"Resized Image (64x64)\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):  # Number of epochs\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Testing loop\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16*16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"model structure\", model)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(model, optimizer, criterion)\n",
    "test_model(model)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "model_file_path = \"best_model2.pth\"\n",
    "torch.save(model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EfficientCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # 1 input channel, 16 filters\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Reduce size by half\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16 * 16, 64),  # Fewer neurons in FC layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "lightModel = EfficientCNN(num_classes).to(device)\n",
    "optimizer = optim.Adam(lightModel.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"lightModel structure\", lightModel)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(lightModel, optimizer, criterion)\n",
    "test_model(lightModel)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),  # Normalize feature maps for faster convergence\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Add a third convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 256),  # More neurons for higher capacity\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Add dropout for regularization\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "deepModel = EnhancedCNN(num_classes).to(device)\n",
    "optimizer = optim.Adam(deepModel.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"lightModel structure\", deepModel)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(deepModel, optimizer, criterion)\n",
    "test_model(deepModel)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "model_file_path = \"deepest_model.pth\"\n",
    "torch.save(deepModel.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two deeper models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_model_with_scheduler(model, optimizer, scheduler, criterion, num_epochs=10):\n",
    "    print(num_epochs)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning Rate: {param_group['lr']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLikeCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetLikeCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Shortcut connection to match the channel dimensions\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=1),  # Match input to output channels\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Ensure consistent spatial dimensions\n",
    "        self.fc = None  # Placeholder, will be dynamically initialized\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))  # First convolution\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))  # Second convolution\n",
    "        residual = self.shortcut(x)              # Adjust dimensions for residual\n",
    "        x = torch.relu(self.bn3(self.conv3(x)) + residual)  # Add residual to output\n",
    "        x = self.pool(x)  # Downsample to fixed size\n",
    "        x = torch.flatten(x, 1)  # Flatten before fully connected layers\n",
    "        x = self.fc(x)  # Fully connected layers\n",
    "        return x\n",
    "\n",
    "    def initialize_fc(self, input_shape):\n",
    "        # Dynamically calculate the input size of the flattened feature map\n",
    "        dummy_input = torch.zeros(input_shape)  # Create a dummy tensor with input shape\n",
    "        conv_output = self.conv1(dummy_input)  # Pass through layers to determine flattened size\n",
    "        conv_output = torch.relu(self.bn1(conv_output))\n",
    "        conv_output = torch.relu(self.bn2(self.conv2(conv_output)))\n",
    "        residual = self.shortcut(conv_output)\n",
    "        conv_output = torch.relu(self.bn3(self.conv3(conv_output)) + residual)\n",
    "        conv_output = self.pool(conv_output)\n",
    "        flattened_size = torch.flatten(conv_output, 1).shape[1]  # Calculate flattened size\n",
    "        \n",
    "        # Define the fully connected layers dynamically\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "# Define number of classes and input shape\n",
    "input_shape = (1, 1, 64, 64)  # Batch size of 1, 1 channel, 64x64 image size\n",
    "\n",
    "# Instantiate the model and initialize fully connected layers\n",
    "deeper1Model = ResNetLikeCNN(num_classes=num_classes)\n",
    "deeper1Model.initialize_fc(input_shape)\n",
    "\n",
    "# Move model to the device\n",
    "deeper1Model = deeper1Model.to(device)\n",
    "\n",
    "# Define optimizer, scheduler, and criterion\n",
    "optimizer = optim.Adam(deeper1Model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print model structure\n",
    "print(\"ResNetLikeCNN structure:\")\n",
    "print(deeper1Model)\n",
    "\n",
    "# Training and testing\n",
    "start_time = time.time()\n",
    "train_model(deeper1Model, optimizer, scheduler, criterion, num_epochs)\n",
    "test_model(deeper1Model)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "# Save the trained model\n",
    "model_file_path = \"deeper1_model_dynamic.pth\"\n",
    "torch.save(deeper1Model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperCNN2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeeperCNN2, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc1 = None  # Placeholder, will be initialized dynamically\n",
    "        self.fc2 = nn.Linear(512, num_classes)  # Output layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)  # Pass through convolutional layers\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch size\n",
    "        x = self.fc1(x)  # First fully connected layer\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "    def initialize_fc(self, input_shape):\n",
    "        # Dynamically calculate the input size of the flattened feature map\n",
    "        dummy_input = torch.zeros(input_shape)  # Create a dummy tensor with input shape\n",
    "        conv_output = self.conv_layers(dummy_input)  # Pass through conv layers\n",
    "        flattened_size = torch.flatten(conv_output, 1).shape[1]  # Calculate flattened size\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)  # Initialize fc1 with calculated size\n",
    "\n",
    "\n",
    "input_shape = (1, 1, 64, 64)  # Batch size of 1, 1 channel, 64x64 image size\n",
    "\n",
    "# Instantiate the model and initialize fully connected layer\n",
    "deeper2Model = DeeperCNN2(num_classes=num_classes)\n",
    "deeper2Model.initialize_fc(input_shape)\n",
    "\n",
    "# Move model to the device\n",
    "deeper2Model = deeper2Model.to(device)\n",
    "\n",
    "# Define optimizer, scheduler, and criterion\n",
    "optimizer = optim.Adam(deeper2Model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print model structure\n",
    "print(\"DeeperCNN2 structure:\")\n",
    "print(deeper2Model)\n",
    "\n",
    "# Training and testing\n",
    "start_time = time.time()\n",
    "train_model(deeper2Model, optimizer, scheduler, criterion, num_epochs)\n",
    "test_model(deeper2Model)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "# Save the trained model\n",
    "model_file_path = \"deeper2_model_dynamic.pth\"\n",
    "torch.save(deeper2Model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
