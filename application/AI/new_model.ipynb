{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/bin/python\n",
      "Requirement already satisfied: pandas in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: ndjson in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (0.3.1)\n",
      "Requirement already satisfied: google-cloud-storage in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (2.37.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (2.24.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-cloud-storage) (1.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (5.29.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.25.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: six>=1.5 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.12.14)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/corentinlaval/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!pip install pandas ndjson google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 12\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement du fichier : full/simplified/star.ndjson\n",
      "Données brutes du fichier full/simplified/star.ndjson (500 premiers caractères) :\n",
      "{\"word\":\"star\",\"countrycode\":\"US\",\"timestamp\":\"2017-03-07 16:39:23.36509 UTC\",\"recognized\":true,\"key_id\":\"5260413706960896\",\"drawing\":[[[67,37,30,44,70,105,116,184,192,194,161,190,237,230,185,155,145,110,102,98,96,78,76,70,0,18,35,48,60],[160,220,243,240,226,200,189,251,255,249,160,145,106,103,103,95,72,9,0,5,35,110,104,101,91,128,154,165,166]]]}\n",
      "{\"word\":\"star\",\"countrycode\":\"US\",\"timestamp\":\"2017-03-15 15:13:18.18952 UTC\",\"recognized\":true,\"key_id\":\"4853913289228288\",\"drawing\":[[[104,106,115,12\n",
      "Fichier chargé avec succès. Nombre d'enregistrements : 137619\n",
      "Téléchargement du fichier : full/simplified/sword.ndjson\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTéléchargement du fichier : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     dataset_df \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_data_and_parse_it\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     all_dfs\u001b[38;5;241m.\u001b[39mappend(dataset_df)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/application/AI/download.py:54\u001b[0m, in \u001b[0;36mdownload_data_and_parse_it\u001b[0;34m(bucket_name, blob_name)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLe fichier \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m est introuvable dans le bucket \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Téléchargement du contenu du fichier\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_as_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Contenu brut sous forme de texte\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDonnées brutes du fichier \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (500 premiers caractères) :\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontent[:\u001b[38;5;241m500\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Parsing des données NDJSON\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/google/cloud/storage/blob.py:1700\u001b[0m, in \u001b[0;36mBlob.download_as_text\u001b[0;34m(self, client, start, end, raw_download, encoding, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, retry)\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;129m@create_trace_span\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorage.Blob.downloadAsText\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdownload_as_text\u001b[39m(\n\u001b[1;32m   1606\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     retry\u001b[38;5;241m=\u001b[39mDEFAULT_RETRY,\n\u001b[1;32m   1620\u001b[0m ):\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download the contents of this blob as text (*not* bytes).\u001b[39;00m\n\u001b[1;32m   1622\u001b[0m \n\u001b[1;32m   1623\u001b[0m \u001b[38;5;124;03m    If :attr:`user_project` is set on the bucket, bills the API request\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;124;03m    :returns: The data stored in this blob, decoded to text.\u001b[39;00m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1700\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_as_bytes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_etag_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_etag_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_etag_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_etag_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdecode(encoding)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/google/cloud/storage/blob.py:1473\u001b[0m, in \u001b[0;36mBlob.download_as_bytes\u001b[0;34m(self, client, start, end, raw_download, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Download the contents of this blob as a bytes object.\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m \n\u001b[1;32m   1385\u001b[0m \u001b[38;5;124;03mIf :attr:`user_project` is set on the bucket, bills the API request\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;124;03m:raises: :class:`google.cloud.exceptions.NotFound`\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m string_buffer \u001b[38;5;241m=\u001b[39m BytesIO()\n\u001b[0;32m-> 1473\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prep_and_do_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstring_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_etag_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_etag_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_etag_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_etag_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m string_buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/google/cloud/storage/blob.py:4423\u001b[0m, in \u001b[0;36mBlob._prep_and_do_download\u001b[0;34m(self, file_obj, client, start, end, raw_download, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, command)\u001b[0m\n\u001b[1;32m   4420\u001b[0m transport \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39m_http\n\u001b[1;32m   4422\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 4423\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4435\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m resumable_media\u001b[38;5;241m.\u001b[39mInvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   4436\u001b[0m     _raise_from_invalid_response(exc)\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/google/cloud/storage/blob.py:1045\u001b[0m, in \u001b[0;36mBlob._do_download\u001b[0;34m(self, transport, file_obj, download_url, headers, start, end, raw_download, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   1039\u001b[0m     download\u001b[38;5;241m.\u001b[39m_retry_strategy \u001b[38;5;241m=\u001b[39m retry_strategy\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_trace_span(\n\u001b[1;32m   1041\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorage.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/consume\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1042\u001b[0m         attributes\u001b[38;5;241m=\u001b[39mextra_attributes,\n\u001b[1;32m   1043\u001b[0m         api_request\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1044\u001b[0m     ):\n\u001b[0;32m-> 1045\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mdownload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_headers_from_download(response)\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/google/resumable_media/requests/download.py:263\u001b[0m, in \u001b[0;36mDownload.consume\u001b[0;34m(self, transport, timeout)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_to_stream(result)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_and_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriable_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_status_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_strategy\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/google/resumable_media/requests/_request_helpers.py:155\u001b[0m, in \u001b[0;36mwait_and_retry\u001b[0;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[1;32m    153\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _CONNECTION_ERROR_CLASSES \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    157\u001b[0m     error \u001b[38;5;241m=\u001b[39m e  \u001b[38;5;66;03m# Fall through to retry, if there are retries left.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/google/resumable_media/requests/download.py:259\u001b[0m, in \u001b[0;36mDownload.consume.<locals>.retriable_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bytes_downloaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_to_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/google/resumable_media/requests/download.py:131\u001b[0m, in \u001b[0;36mDownload._write_to_stream\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    127\u001b[0m local_checksum_object \u001b[38;5;241m=\u001b[39m _add_decoder(response\u001b[38;5;241m.\u001b[39mraw, checksum_object)\n\u001b[1;32m    128\u001b[0m body_iter \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    129\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m_request_helpers\u001b[38;5;241m.\u001b[39m_SINGLE_GET_CHUNK_SIZE, decode_unicode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    130\u001b[0m )\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m body_iter:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/TSE/FISE3/BigData/BigData/.venv/lib/python3.10/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ndjson\n",
    "from google.cloud import storage\n",
    "from download import download_data_and_parse_it\n",
    "\n",
    "bucket_name = \"quickdraw_dataset\"\n",
    "\n",
    "datasets = [\"star\", \"sword\", \"tent\", \"apple\", \"banana\", \"cat\", \n",
    "        \"dog\", \"car\", \"house\", \"tree\", \"guitar\", \"bicycle\"]\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    file_path = f\"full/simplified/{dataset}.ndjson\"\n",
    "    \n",
    "    print(f\"Téléchargement du fichier : {file_path}\")\n",
    "    try:\n",
    "        dataset_df = download_data_and_parse_it(bucket_name,file_path)\n",
    "        all_dfs.append(dataset_df)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erreur : {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur inattendue : {e}\")\n",
    "\n",
    "# Combine all datasets\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Assign class labels using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['class'] = le.fit_transform(df['word'])\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)  # 60% train, 40% temp\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # Split temp into 20% val, 20% test\n",
    "\n",
    "print(\"size of datasets array :\", len(datasets))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Vector-To-Image Algorithm - Drawing-to-image changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, drawings, labels, resize_to=(64, 64)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            drawings (list or array): List of drawing data (tensor or numpy arrays).\n",
    "            labels (list or array): List of class labels corresponding to each drawing.\n",
    "            resize_to (tuple): Target size for resizing the image.\n",
    "        \"\"\"\n",
    "        self.drawings = drawings\n",
    "        self.labels = labels\n",
    "        self.resize_to = resize_to  # Tuple (width, height)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.drawings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the drawing format to image\n",
    "        drawing = self.drawings.iloc[idx] if isinstance(self.drawings, pd.Series) else self.drawings[idx]\n",
    "        image = self.drawing_to_image(drawing)\n",
    "        label = self.labels.iloc[idx] if isinstance(self.labels, pd.Series) else self.labels[idx]\n",
    "        \n",
    "        # Convert the image to a tensor and add batch dimension\n",
    "        image_tensor = torch.FloatTensor(image).unsqueeze(0)\n",
    "        return image_tensor, label\n",
    "\n",
    "    def drawing_to_image(self, drawing):\n",
    "        # Create a blank 256x256 image\n",
    "        img_size = 256\n",
    "        image = np.zeros((img_size, img_size), dtype=np.uint8)\n",
    "        \n",
    "        # Iterate through the strokes\n",
    "        for stroke in drawing:\n",
    "            x_coords = stroke[0]\n",
    "            y_coords = stroke[1]\n",
    "\n",
    "            # For each stroke, draw lines between consecutive points\n",
    "            for i in range(len(x_coords) - 1):\n",
    "                x1, y1 = int(x_coords[i]), int(y_coords[i])\n",
    "                x2, y2 = int(x_coords[i + 1]), int(y_coords[i + 1])\n",
    "\n",
    "                # Ensure that coordinates stay within bounds\n",
    "                x1 = max(0, min(x1, img_size - 1))\n",
    "                y1 = max(0, min(y1, img_size - 1))\n",
    "                x2 = max(0, min(x2, img_size - 1))\n",
    "                y2 = max(0, min(y2, img_size - 1))\n",
    "\n",
    "                # Draw line on image (we simply put the endpoints as pixels here)\n",
    "                image[y1, x1] = 255\n",
    "                image[y2, x2] = 255\n",
    "        \n",
    "        # Resize the image to the desired size (64x64)\n",
    "        pil_image = Image.fromarray(image)  # Convert to PIL Image\n",
    "        pil_image = pil_image.resize(self.resize_to, Image.Resampling.LANCZOS)  # Resize image\n",
    "        \n",
    "        # Convert back to numpy array\n",
    "        resized_image = np.array(pil_image)\n",
    "        return resized_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create datasets\n",
    "# train_dataset = QuickDrawDataset(train_df['drawing'], train_df['class'])\n",
    "# val_dataset = QuickDrawDataset(val_df['drawing'], val_df['class'])\n",
    "# test_dataset = QuickDrawDataset(test_df['drawing'], test_df['class'])\n",
    "\n",
    "# # Create DataLoaders\n",
    "# from torch.utils.data import DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the datasets with resizing\n",
    "train_dataset = QuickDrawDataset(train_df['drawing'], train_df['class'], resize_to=(64, 64))\n",
    "val_dataset = QuickDrawDataset(val_df['drawing'], val_df['class'], resize_to=(64, 64))\n",
    "test_dataset = QuickDrawDataset(test_df['drawing'], test_df['class'], resize_to=(64, 64))\n",
    "\n",
    "# Create DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32  # Example batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to visualize some entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]]), np.int64(0))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drawing = train_dataset.__getitem__(300)\n",
    "\n",
    "print(drawing)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the image\n",
    "plt.imshow(drawing[0].squeeze(), cmap='gray')  # Squeeze to remove the single channel dimension\n",
    "plt.axis('off')  # Hide axis for better visualization\n",
    "plt.title(drawing[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example input data (simplified vector data from the NDJSON)\n",
    "drawings = [\n",
    "    [[[4,18,29,63,93,120,146,169,186,218,244,234,186,154,128,86,44,14,0],[7,51,66,90,101,106,106,101,93,67,22,23,49,58,59,53,26,16,6]],[[10,27,42,78,135,162,212,230,244],[15,39,53,67,80,74,48,35,20]],[[9,2,16,22,23,20],[18,3,0,3,8,20]],[[229,244,254,252,241],[23,17,18,22,30]],[[52,52],[52,52]],[[52,50],[52,52]],[[59,43],[69,61]]],\n",
    "    [[[223,226,227,233,254,255,248,227],[35,25,2,0,2,21,27,30]],[[234,235,244,246,249],[4,8,12,19,20]],[[232,208,168,135,98,37],[28,51,104,138,164,196]],[[255,255,245,194,182,172,115,26,18,8],[25,61,106,199,213,217,219,235,235,230]],[[12,1,0,0,10,22,29,28,16,11],[193,191,194,206,219,218,206,199,194,197]],[[41,29,12],[232,228,217]],[[7,7,12,14,14,18,19,23,27,29,37],[186,199,190,193,209,200,204,204,194,203,192]],[[230,230,235,238,241,245,245],[8,15,8,17,16,4,11]],[[251,251,241,235,211,191,121,59,31],[26,69,113,126,155,168,200,215,215]],[[214,201,177,153,120,108],[72,102,131,152,167,175]]], \n",
    "    [[[2,0,4,24,41,94,160,186,189,177,151,134,94,71,30],[0,38,79,138,162,207,247,255,251,238,218,200,139,98,45]]]\n",
    "]\n",
    "\n",
    "# Reconstruct the image from the simplified drawing\n",
    "index = 0\n",
    "for drawing in drawings:\n",
    "    reconstructed_image = train_dataset.drawing_to_image(drawing)\n",
    "    index+=1\n",
    "    # Display the image\n",
    "    plt.imshow(reconstructed_image, cmap='gray')\n",
    "    plt.title(index)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just a resizing test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "# Assuming `drawing` is the tuple as described\n",
    "image_tensor = drawing[0]  # Image tensor of shape (1, H, W)\n",
    "label = drawing[1]         # The label (in this case, 3)\n",
    "\n",
    "# Convert the image to 3D (H, W, C) for easier manipulation\n",
    "image_numpy = image_tensor.squeeze().cpu().numpy()\n",
    "\n",
    "# Resize the image to a smaller size using a transformation (e.g., 128x128)\n",
    "resize_transform = T.Resize((64, 64))  # Resize to 128x128\n",
    "resized_image_tensor = resize_transform(image_tensor.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "# Convert back to numpy array for visualization\n",
    "resized_image_numpy = resized_image_tensor.squeeze().cpu().numpy()\n",
    "\n",
    "# Visualize the original and resized image\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original image\n",
    "ax[0].imshow(image_numpy, cmap='gray')\n",
    "ax[0].set_title(\"Original Image (256x256)\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "# Resized image\n",
    "ax[1].imshow(resized_image_numpy, cmap='gray')\n",
    "ax[1].set_title(\"Resized Image (64x64)\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):  # Number of epochs\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Testing loop\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16*16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"model structure\", model)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(model, optimizer, criterion)\n",
    "test_model(model)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "model_file_path = \"best_model2.pth\"\n",
    "torch.save(model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EfficientCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # 1 input channel, 16 filters\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # Reduce size by half\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16 * 16, 64),  # Fewer neurons in FC layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "lightModel = EfficientCNN(num_classes).to(device)\n",
    "optimizer = optim.Adam(lightModel.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"lightModel structure\", lightModel)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(lightModel, optimizer, criterion)\n",
    "test_model(lightModel)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EnhancedCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),  # Normalize feature maps for faster convergence\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Add a third convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 256),  # More neurons for higher capacity\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Add dropout for regularization\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "deepModel = EnhancedCNN(num_classes).to(device)\n",
    "optimizer = optim.Adam(deepModel.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"lightModel structure\", deepModel)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_model(deepModel, optimizer, criterion)\n",
    "test_model(deepModel)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "model_file_path = \"deepest_model.pth\"\n",
    "torch.save(deepModel.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two deeper models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def train_model_with_scheduler(model, optimizer, scheduler, criterion, num_epochs=10):\n",
    "    print(num_epochs)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning Rate: {param_group['lr']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLikeCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetLikeCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Shortcut connection to match the channel dimensions\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=1),  # Match input to output channels\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Ensure consistent spatial dimensions\n",
    "        self.fc = None  # Placeholder, will be dynamically initialized\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))  # First convolution\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))  # Second convolution\n",
    "        residual = self.shortcut(x)              # Adjust dimensions for residual\n",
    "        x = torch.relu(self.bn3(self.conv3(x)) + residual)  # Add residual to output\n",
    "        x = self.pool(x)  # Downsample to fixed size\n",
    "        x = torch.flatten(x, 1)  # Flatten before fully connected layers\n",
    "        x = self.fc(x)  # Fully connected layers\n",
    "        return x\n",
    "\n",
    "    def initialize_fc(self, input_shape):\n",
    "        # Dynamically calculate the input size of the flattened feature map\n",
    "        dummy_input = torch.zeros(input_shape)  # Create a dummy tensor with input shape\n",
    "        conv_output = self.conv1(dummy_input)  # Pass through layers to determine flattened size\n",
    "        conv_output = torch.relu(self.bn1(conv_output))\n",
    "        conv_output = torch.relu(self.bn2(self.conv2(conv_output)))\n",
    "        residual = self.shortcut(conv_output)\n",
    "        conv_output = torch.relu(self.bn3(self.conv3(conv_output)) + residual)\n",
    "        conv_output = self.pool(conv_output)\n",
    "        flattened_size = torch.flatten(conv_output, 1).shape[1]  # Calculate flattened size\n",
    "        \n",
    "        # Define the fully connected layers dynamically\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "# Define number of classes and input shape\n",
    "input_shape = (1, 1, 64, 64)  # Batch size of 1, 1 channel, 64x64 image size\n",
    "\n",
    "# Instantiate the model and initialize fully connected layers\n",
    "deeper1Model = ResNetLikeCNN(num_classes=num_classes)\n",
    "deeper1Model.initialize_fc(input_shape)\n",
    "\n",
    "# Move model to the device\n",
    "deeper1Model = deeper1Model.to(device)\n",
    "\n",
    "# Define optimizer, scheduler, and criterion\n",
    "optimizer = optim.Adam(deeper1Model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print model structure\n",
    "print(\"ResNetLikeCNN structure:\")\n",
    "print(deeper1Model)\n",
    "\n",
    "# Training and testing\n",
    "start_time = time.time()\n",
    "train_model(deeper1Model, optimizer, scheduler, criterion, num_epochs)\n",
    "test_model(deeper1Model)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "# Save the trained model\n",
    "model_file_path = \"deeper1_model_dynamic.pth\"\n",
    "torch.save(deeper1Model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperCNN2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeeperCNN2, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc1 = None  # Placeholder, will be initialized dynamically\n",
    "        self.fc2 = nn.Linear(512, num_classes)  # Output layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)  # Pass through convolutional layers\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch size\n",
    "        x = self.fc1(x)  # First fully connected layer\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "    def initialize_fc(self, input_shape):\n",
    "        # Dynamically calculate the input size of the flattened feature map\n",
    "        dummy_input = torch.zeros(input_shape)  # Create a dummy tensor with input shape\n",
    "        conv_output = self.conv_layers(dummy_input)  # Pass through conv layers\n",
    "        flattened_size = torch.flatten(conv_output, 1).shape[1]  # Calculate flattened size\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)  # Initialize fc1 with calculated size\n",
    "\n",
    "\n",
    "input_shape = (1, 1, 64, 64)  # Batch size of 1, 1 channel, 64x64 image size\n",
    "\n",
    "# Instantiate the model and initialize fully connected layer\n",
    "deeper2Model = DeeperCNN2(num_classes=num_classes)\n",
    "deeper2Model.initialize_fc(input_shape)\n",
    "\n",
    "# Move model to the device\n",
    "deeper2Model = deeper2Model.to(device)\n",
    "\n",
    "# Define optimizer, scheduler, and criterion\n",
    "optimizer = optim.Adam(deeper2Model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Print model structure\n",
    "print(\"DeeperCNN2 structure:\")\n",
    "print(deeper2Model)\n",
    "\n",
    "# Training and testing\n",
    "start_time = time.time()\n",
    "train_model(deeper2Model, optimizer, scheduler, criterion, num_epochs)\n",
    "test_model(deeper2Model)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "# Save the trained model\n",
    "model_file_path = \"deeper2_model_dynamic.pth\"\n",
    "torch.save(deeper2Model.state_dict(), model_file_path)\n",
    "print(f\"Model saved to {model_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
